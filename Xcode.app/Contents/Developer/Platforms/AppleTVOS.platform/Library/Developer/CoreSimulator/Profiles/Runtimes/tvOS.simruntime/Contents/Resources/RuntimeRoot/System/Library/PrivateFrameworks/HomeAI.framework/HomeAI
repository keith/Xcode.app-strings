@(#)PROGRAM:HomeAI  PROJECT:HomeAI-141.2.4
fffff
Mb@?
UsesIOSurface
HardwareAcceleratedTransfer
HighSpeedTransfer
HMIPixelBufferTransferMultiStageResize
?a0cTa1cTa2cTa3cTa0hTa1hTa2hTa3hT
Motion
Person
Vehicle
sFXg@
+Sg@
?>Ip
?(-_U
=$@'
hp|i1
f?94{W
A`~q@
#>m?
?>Ip
iUK:
\.@;&b|^
hp|i1
UUUUUU
?333333
N6homeai3mod28ImageDescriptorBufferFloat32E
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
N6homeai10clustering15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorIxNS_9allocatorIxEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClusterer9cluster_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJxxfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
N6homeai3mod29ImageDescriptorBufferAbstractE
?ffffff
GNSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClustererENS_9allocatorIS3_EEEE
MbP?
]=dZ
>X@[;
Bhs<
m%*=
%pP=2.
Motion
Person
Vehicle
Face
N2cv13BaseRowFilterE
N2cv16BaseColumnFilterE
N2cv10BaseFilterE
N2cv12FilterEngineE
N2cv18SymmRowSmallFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv9RowFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv18SymmRowSmallFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIhiNS_12RowVec_8u32sEEE
N2cv9RowFilterIhfNS_8RowNoVecEEE
N2cv9RowFilterIhdNS_8RowNoVecEEE
N2cv9RowFilterItfNS_8RowNoVecEEE
N2cv9RowFilterItdNS_8RowNoVecEEE
N2cv9RowFilterIsfNS_13RowVec_16s32fEEE
N2cv9RowFilterIsdNS_8RowNoVecEEE
N2cv9RowFilterIffNS_10RowVec_32fEEE
N2cv9RowFilterIfdNS_8RowNoVecEEE
N2cv9RowFilterIddNS_8RowNoVecEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv16SymmColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv21SymmColumnSmallFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv12ColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv21SymmColumnSmallFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv12ColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv16SymmColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv8Filter2DIhNS_4CastIfhEENS_12FilterVec_8uEEE
N2cv8Filter2DIhNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIfsEENS_15FilterVec_8u16sEEE
N2cv8Filter2DIhNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIfNS_4CastIffEENS_13FilterVec_32fEEE
N2cv8Filter2DIdNS_4CastIddEENS_11FilterNoVecEEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
ucwsifdr
?qxs?qxs
9D)5:
@`|-A
PF SdF
 [@W:[
D]eV
"x@
z4I5z
?~my
!Y?4
!I?u
!)?gX
?g^FT
?$"LT
?$"LT
N2cv15ThresholdRunnerE
N2cv6detail16LKTrackerInvokerE
N2cv9ColumnSumIihEE
N2cv9ColumnSumIitEE
N2cv9ColumnSumIisEE
N2cv6RowSumIhiEE
N2cv6RowSumIhdEE
N2cv6RowSumItiEE
N2cv6RowSumItdEE
N2cv6RowSumIsiEE
N2cv6RowSumIiiEE
N2cv6RowSumIsdEE
N2cv6RowSumIfdEE
N2cv6RowSumIddEE
N2cv9ColumnSumIdhEE
N2cv9ColumnSumIdtEE
N2cv9ColumnSumIdsEE
N2cv9ColumnSumIiiEE
N2cv9ColumnSumIifEE
N2cv9ColumnSumIdfEE
N2cv9ColumnSumIidEE
N2cv9ColumnSumIddEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
<N2cv11_InputArrayE
N2cv12_OutputArrayE
N2cv9ExceptionE
14EmptyFuncTable
12GpuFuncTable
N2cv16ParallelLoopBodyE
N2cv16MorphologyRunnerE
N2cv14MorphRowFilterINS_5MinOpIhEENS_12MorphRowIVecINS_6VMin8uEEEEE
N2cv14MorphRowFilterINS_5MinOpItEENS_12MorphRowIVecINS_7VMin16uEEEEE
N2cv14MorphRowFilterINS_5MinOpIsEENS_12MorphRowIVecINS_7VMin16sEEEEE
N2cv14MorphRowFilterINS_5MinOpIfEENS_12MorphRowFVecINS_7VMin32fEEEEE
N2cv14MorphRowFilterINS_5MinOpIdEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIhEENS_12MorphRowIVecINS_6VMax8uEEEEE
N2cv14MorphRowFilterINS_5MaxOpItEENS_12MorphRowIVecINS_7VMax16uEEEEE
N2cv14MorphRowFilterINS_5MaxOpIsEENS_12MorphRowIVecINS_7VMax16sEEEEE
N2cv14MorphRowFilterINS_5MaxOpIfEENS_12MorphRowFVecINS_7VMax32fEEEEE
N2cv14MorphRowFilterINS_5MaxOpIdEENS_13MorphRowNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIhEENS_15MorphColumnIVecINS_6VMin8uEEEEE
N2cv17MorphColumnFilterINS_5MinOpItEENS_15MorphColumnIVecINS_7VMin16uEEEEE
N2cv17MorphColumnFilterINS_5MinOpIsEENS_15MorphColumnIVecINS_7VMin16sEEEEE
N2cv17MorphColumnFilterINS_5MinOpIfEENS_15MorphColumnFVecINS_7VMin32fEEEEE
N2cv17MorphColumnFilterINS_5MinOpIdEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIhEENS_15MorphColumnIVecINS_6VMax8uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpItEENS_15MorphColumnIVecINS_7VMax16uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIsEENS_15MorphColumnIVecINS_7VMax16sEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIfEENS_15MorphColumnFVecINS_7VMax32fEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIdEENS_16MorphColumnNoVecEEE
N2cv11MorphFilterINS_5MinOpIhEENS_9MorphIVecINS_6VMin8uEEEEE
N2cv11MorphFilterINS_5MinOpItEENS_9MorphIVecINS_7VMin16uEEEEE
N2cv11MorphFilterINS_5MinOpIsEENS_9MorphIVecINS_7VMin16sEEEEE
N2cv11MorphFilterINS_5MinOpIfEENS_9MorphFVecINS_7VMin32fEEEEE
N2cv11MorphFilterINS_5MinOpIdEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIhEENS_9MorphIVecINS_6VMax8uEEEEE
N2cv11MorphFilterINS_5MaxOpItEENS_9MorphIVecINS_7VMax16uEEEEE
N2cv11MorphFilterINS_5MaxOpIsEENS_9MorphIVecINS_7VMax16sEEEEE
N2cv11MorphFilterINS_5MaxOpIfEENS_9MorphFVecINS_7VMax32fEEEEE
N2cv11MorphFilterINS_5MaxOpIdEENS_10MorphNoVecEEE
?N2cv5MatOpE
N2cv14MatOp_IdentityE
N2cv11MatOp_AddExE
N2cv9MatOp_BinE
N2cv9MatOp_CmpE
N2cv10MatOp_GEMME
N2cv12MatOp_InvertE
N2cv7MatOp_TE
N2cv11MatOp_SolveE
N2cv17MatOp_InitializerE
Error creating directory for person:%@
version
UUID
displayName
person.json
Error saving metadata to disk for person:%@
v8@?0
Error creating directory for person face crop:%@
faceBoundingBox
dateCreated
facecrop.json
Error saving metadata to disk for person face crop:%@
facecrop.jpeg
Error saving face crop image to disk for person face crop:%@
external.person.datasource.disk
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
HMIMutableCluster
centroid
T@"HMIDESMutableFloatArray",R,V_centroid
TQ,R,N
faceprintUUIDs
T@"NSArray",R
linkedEntityUUIDs
T@"NSSet",R
featureNames
T@"NSSet",R,N
pixelBuffer
T^{__CVBuffer=},R,V_pixelBuffer
inputName
T@"NSString",R,V_inputName
image__Placeholder__0
ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
SignificantActivityDetector
CoreML output nil or not of type MLFeatureTypeMultiArray
SignificantActivity
mlmodelc
significant.activity.detector
mlModel
T@"MLModel",R,V_mlModel
inputFeatureValueName
T@"NSString",R,V_inputFeatureValueName
offsetsFeatureValueNames
T@"NSArray",R,V_offsetsFeatureValueNames
scoresFeatureValueNames
T@"NSArray",R,V_scoresFeatureValueNames
yawsFeatureValueNames
T@"NSArray",R,V_yawsFeatureValueNames
rollsFeatureValueNames
T@"NSArray",R,V_rollsFeatureValueNames
nmsConfiguration
T@"HMINMSConfiguration",R,V_nmsConfiguration
useSoftmax
TB,R,V_useSoftmax
predictionOptions
T@"MLPredictionOptions",R,V_predictionOptions
inputDimensions
T{CGSize=dd},R,V_inputDimensions
HMISystemResourceUsageLevelLow
HMISystemResourceUsageLevelMed
HMISystemResourceUsageLevelHigh
HMISystemResourceUsageLevelUnknown
HMISystemResourceUsageLevelUndefined
systemResourceUsageLevel
Tq,N,V_systemResourceUsageLevel
delegate
T@"<HMISystemResourceUsageMonitorDelegate>",W
systemResourceUsageMonitorImpl
T@"HMISystemResourceUsageMonitorImpl",R,V_systemResourceUsageMonitorImpl
workQueue
T@"NSObject<OS_dispatch_queue>",R,V_workQueue
FaceFilteredState
HMIVAEF.fr
HMIVAEF.ya
HMIVAEF.ro
None
QualitySVMKnown
QualitySVMUnknown
QualityANFRScore
NoPredictions
HasFaceMask
Confidence
Bounding Box
Face Recognition
Face Yaw
Face Roll
%@@(%@,%@)
HMIVideoAnalyzerEventFace
T@"NSNumber",R,V_yaw
roll
T@"NSNumber",R,V_roll
faceRecognition
T@"HMIFaceRecognition",R,V_faceRecognition
%c%c%c%c
Type: %@, DTS: %@, PTS: %@, DUR: %@, NUM: %ld [%@]
Sync
%c%02d:%02d:%02d.%03d
%*lld/%d %@
%*lld %@
POSITIVE INFINITY
NEGATIVE INFINITY
   INDEFINITE    
  INVALID TIME   
%@ DTS %@ PTS %@ dur %@%@
PTS: %.2f
Bogus atomSize %llu, recovering by adjusting size.
en_US_POSIX
yyyy-MM-dd'T'HH:mm:ss
%.3f x %.3f
%.3f, %.3f %@
%.2fs
%02x
time
T{?=qiIq},R
T{?=qiIq},R,V_time
value
T@,R,V_value
q24@?0@"<HMIVideoEvent>"8@"<HMIVideoEvent>"16
B32@?0@"<HMIVideoEvent>"8Q16^B24
@16@?0@"<HMIVideoEvent>"8
Time: %@, Date: %@
date
T@"NSDate",R,V_date
lock
HMIVideoAnalyzerEventMotion
HMICoreAnalyticsVIPModelReportTime
Received nil data source
Fetching settings using data source: %@
Error fetching settings: %@
v24@?0@"HMIHomePersonManagerSettings"8@"NSError"16
handleUpdatedPerson: %@
handleUpdatedUnassociatedFaceCrop: %@
handleUpdatedPersonFaceCrop: %@
handleUpdatedFaceprint: %@
handleUpdatedSettings: %@
handleRemovedPersonWithUUID: %@
handleRemovedFaceCropWithUUID: %@
handleRemovedFaceprintWithUUID: %@
Successfully handled face misclassification
Error in handling face misclassification, error:%@
v24@?0@"NSDictionary"8@"NSError"16
Submitted face misclassification task, taskID:%u
B16@?0@"HMIFaceClassification"8
Not storing face crop and faceprint for HMIHomePersonManager.UUID:%@ from face event:%@
Storing unknown to Home face crop:%@ and faceprint:%@
Error storing unassociated face crop:%@, error:%@
Stored unassociated face crop:%@
v16@?0@"NSError"8
Error storing faceprint:%@, error:%@
Stored faceprint:%@
Reached face crop limit for sessionEntityUUID:%@ for HMIHomePersonManager.UUID:%@; not storing
Timer fired, but person data is not yet available, waiting...
Timer fired, updating home persons model
Not triggering daily VIP Model Core Analytics event, last event was sent less than 1 day ago
Triggering daily VIP Model Core Analytics event
Successfully ran persons model summary task
Failed to run persons model summary task, error:%@
Submitted persons model summary task, taskID:%u
Unrecognized timer: %@
UUID:%@ HomeUUID:%@
home.person.manager
Updating with settings: %@
Settings have disabled face classification, removing home persons model
Settings have enabled face classification, updating home persons model
operationQueue
T@"NSOperationQueue",R,V_operationQueue
watchdogTimer
T@"HMFTimer",R,V_watchdogTimer
analyticsTimer
T@"HMFTimer",R,V_analyticsTimer
T@"HMFUnfairLock",R,N,V_lock
unknownFacesSavedCounts
T@"NSMutableDictionary",R,V_unknownFacesSavedCounts
settings
T@"HMIHomePersonManagerSettings",R,V_settings
dataSource
T@"<HMIHomePersonManagerDataSource>",W,N,V_dataSource
Storing face crop:%@ failed with error:%@
Storing face crop:%@ completed successfully
store.facecrop.operation
T@"<HMIHomePersonManagerDataSource>",R,V_dataSource
faceCrop
T@"HMIFaceCrop",R,V_faceCrop
Storing faceprint:%@ failed with error:%@
Storing faceprint:%@ completed successfully
store.faceprint.operation
faceprint
T@"HMIFaceprint",R,V_faceprint
JPEGRepresentation
Frame %lu @ %@
camera.video.frame
jpegData
T@"NSData",R,V_jpegData
motionDetections
T@"NSArray",&,V_motionDetections
sessionPresentationTime
T{?=qiIq},V_sessionPresentationTime
presentationTime
T{?=qiIq},R,V_presentationTime
size
T{CGSize=dd},R,V_size
frameId
TQ,R,V_frameId
fragmentSequenceNumber
TQ,R,V_fragmentSequenceNumber
Logi Circle 2
%#{flags}
transferPixelBuffer
VTPixelTransferSessionCreate failed. Error %d
VTSessionSetProperty failed. Error %d
VTPixelTransferSessionTransferImage failed. Error %d
JPEGDataFromPixelBuffer
Using 720p params for dewarping
Using 1080p params for dewarping
300p skipping dewarp params
Error creating directory:%@ to save video frames
%@-%06lu.%@
Error saving video frame:%@ JPEG to disk
Saved video frame:%@ to disk
Invalid crop for affine transform
Error in pixelbuffer format for affine transform
Error generating pixelbuffer for affine transform
Error applying affine transform
vision.utilities
Registering for Thermal Level Notifications
v12@?0i8
services
T@"NSMutableDictionary",R,V_services
thermalLevel
TQ,R,V_thermalLevel
/private/var
Cannot get available space, error: %@
Footprint: %@, Average: %@, Peak: %@
OutOfMemory
Reached high water mark.
memory.sampler
tick
T@"HMFTimer",R,V_tick
average
T@"MovingAverage",R,V_average
highWaterMark
Tq,V_highWaterMark
Registering for PBSSystemLoad Notifications
com.apple.appletv.system-load
PBSSystemLoad is now: %llu
HMIPBSSystemLoadMonitor
pbsSystemLoad
TQ,R,V_pbsSystemLoad
PrimaryUsagePage
PrimaryUsage
LocationID
Failed to read sample buffer, error: %@
Asset reader failed, ignoring
HMIVideoAssetReader
HMIHomeKitClient HomeKit Delegate Queue
B16@?0@"HMHome"8
B16@?0@"HMResidentDevice"8
personManager is nil for homeUUID: %@
B16@?0@"HMCameraProfile"8
v16@?0@"HMHomeManager"8
Error refreshing home data: %@
No homes were located
Found home: name: %@, primary: %s, UUID: %@
homekit.client
homeKitOperationQueue
T@"NSOperationQueue",R,V_homeKitOperationQueue
setup
TB,R,GisSetup,V_setup
cachePolicy
TQ,R,V_cachePolicy
homes
T@"NSArray",R,V_homes
didUpdateHomes
T@?,C,V_didUpdateHomes
HMMutableHomeManagerConfiguration
Unable to find class %s
/System/Library/Frameworks/HomeKit.framework/HomeKit
/System/Library/Frameworks/HomeKit.framework/Contents/MacOS/HomeKit
HMHomeManager
Not supported error
General error
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
FaceID
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
FaceprintAndAttributes
FaceQuality
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
HMISignpost
%@ (%@)
beginDate
T@"NSDate",R,V_beginDate
signpostIdentifier
TQ,R,V_signpostIdentifier
identifier
T@"NSUUID",R,C,V_identifier
name
T@"NSString",R,C,V_name
input
T@"MLMultiArray",&,N,V_input
transformed_features
classProbability
FaceQualityFilterSVM
FaceQualityFilterSVMDataScaler
facequality.filter.svm
scalerModel
T@"MLModel",R,V_scalerModel
Tq,R
error
T@"NSError",R
Ignoring %@
You must override %@ in a subclass
task
T@"HMIDESBackgroundTask",&,V_task
fetchAllPersonsWithCompletion
@16@?0@"HMPerson"8
v24@?0@"NSSet"8@"NSError"16
fetchPersonsWithUUIDs:%@
fetchAllPersonFaceCropsWithCompletion
@16@?0@"HMPersonFaceCrop"8
fetchFaceCropsForPersonsWithUUIDs:%@
fetchAllFaceprintsWithCompletion
@16@?0@"HMFaceprint"8
fetchFaceprintsForFaceCropsWithUUIDs:%@
fetchSettingsWithCompletion
performCloudPullWithCompletion
addFaceprints:%@
@16@?0@"HMIFaceprint"8
removeFaceprintsWithUUIDs:%@
external.person.datasource.homekit
photosPersonManager
T@"HMPhotosPersonManager",&,V_photosPersonManager
HMFaceprint
com.apple.cvml.%@
CVML module = %@
HMIFC.ck.pu
dataRepresentation
personUUID
Person UUID
Could not initialize from decoded personUUID: %@
supportsSecureCoding
TB,R
T@"NSUUID",R,C,V_personUUID
HMIVideoAnalyzerFrameResult
{events: [%@], frame: %@}
B16@?0@"HMIVideoAnalyzerEvent"8
v16@?0@"HMIVideoAnalyzerEvent"8
@16@?0#8
frame
T@"HMIVideoFrame",R,V_frame
events
T@"NSSet",R,V_events
hmi://in-memory
HMIMemoryAVAsset
 Fullfilled content request: %@
Fullfilled data request: %@
tracks
T@"NSNumber",R,V_value
Invalid parameter for windowSize: %lu, windowSize must be > 0
v16@?0@"MovingAverageEntry"8
queue
T@"NSMutableArray",&,N,V_queue
windowSize
TQ,R,N,V_windowSize
movingAverage
Td,V_movingAverage
HKD://
analyzed-video-frames
face-classifications
Success
SystemResourceUsageHigh
InsufficentTime
AnyMotion
SkippedAnalysis
Medium
High
No HMIVideoAnalyzerEventClass exists for event type %@
DidAnalyze
DidNotAnalyze
DidFailAnalysis
Canceled
Bypassed
Expired
SessionEnded
InErrorState
Predict
@"NSArray"16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMIVideoAnalyzerEvent"8
duration
T{?=qiIq},V_duration
creationDate
T@"NSDate",&,V_creationDate
lastSequenceNumber
TQ,R,V_lastSequenceNumber
Tq,R,V_events
annotationScores
T@"NSDictionary",R,V_annotationScores
posterFrames
T@"NSArray",R,V_posterFrames
frameResults
T@"NSArray",R,V_frameResults
resultCode
Tq,V_resultCode
timeToAnalyzeFragment
Td,V_timeToAnalyzeFragment
timeSinceFragmentWasSubmitted
Td,V_timeSinceFragmentWasSubmitted
videoFragment
T@"HMICameraVideoFragment",&,V_videoFragment
analysisFPS
Tf,V_analysisFPS
Unknown
local
remote-fragment
serviceType
TQ,V_serviceType
startingMediaIntegritySequenceNumber
TQ,V_startingMediaIntegritySequenceNumber
useScheduler
TB,V_useScheduler
inMediaAnalysis
TB,V_inMediaAnalysis
faceClassificationEnabled
TB,V_faceClassificationEnabled
currentSessionDuration
T{?=qiIq},V_currentSessionDuration
homeUUID
T@"NSUUID",&,V_homeUUID
posterFrameGenerationInterval
TQ,R,V_posterFrameGenerationInterval
posterFrameHeight
TQ,R,V_posterFrameHeight
maxFragmentAnalysisDuration
Td,R,V_maxFragmentAnalysisDuration
maxFragmentDuration
Td,R,V_maxFragmentDuration
transcodeFragment
TB,V_transcodeFragment
camera
T@"HMICamera",R,V_camera
videoFrame
T@"HMICameraVideoFrame",R,V_videoFrame
T@"HMICameraVideoFrame",R,V_frame
detections
T@"NSArray",R,V_detections
regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_regionOfInterest
faceClassifications
T@"NSSet",R,V_faceClassifications
analyzerEvents
T@"NSSet",R,V_analyzerEvents
fragment/%lu
fragmentData
T@"NSMutableData",R
T@"NSURL",&,N,V_url
sequenceNumber
TQ,R,V_sequenceNumber
T@"NSData",R,V_data
moovFragment
T@"NSData",R,N,V_moovFragment
eventTypes
Tq,R,V_eventTypes
activityZones
T@"NSArray",R,V_activityZones
[%@] (%lu) repetitions: %lu/%lu/%lu predictions: %lu/%lu/%lu analyzers: %lu lastEvents: %@ shouldPredict: %@
camera.video.analyzer.history
predictions
Tq,V_predictions
repetitions
Tq,V_repetitions
totalPredictions
Tq,V_totalPredictions
totalRepetitions
Tq,V_totalRepetitions
totalRequests
Tq,V_totalRequests
lastRequestResult
T@"HMICameraVideoAnalyzerResult",&,V_lastRequestResult
lastRequestSignificantEvents
T@"NSArray",&,V_lastRequestSignificantEvents
analyzer
T@"HMICameraVideoAnalyzer",R,W,V_analyzer
minRepetitions
Tq,R,V_minRepetitions
maxPredictions
Tq,R,V_maxPredictions
Start analysis, elapsed time since submission: %fs
v24@?0@"HMIVideoAssetWriter"8@"NSData"16
v72@?0@"HMIVideoAssetWriter"8@"NSData"16{?={?=qiIq}{?=qiIq}}24
%@, Date: %@, Time: %@, BitRate: %ld
v16@?0@"HMICameraActivityZone"8
%@ Fragment:%lu
camera.video.analyzer.request
significantEventsInternal
T@"NSMutableArray",R,V_significantEventsInternal
phase
Tq,V_phase
flag
Tq,V_flag
analysisSubmissionTime
T@"NSDate",R,V_analysisSubmissionTime
timeSinceAnalysisSubmission
Td,R
analysisStartTime
T@"NSDate",R,V_analysisStartTime
timeSinceAnalysisStart
maxAnalysisFPS
Td,R,V_maxAnalysisFPS
Td,R,V_analysisFPS
fragment
T@"HMICameraVideoFragment",R,V_fragment
attributes
T@"HMICameraVideoResourceAttributes",R,V_attributes
encoder
T@"HMIVideoEncoder",R,V_encoder
retimer
T@"HMIVideoRetimer",R,V_retimer
frameSampler
T@"HMIVideoFrameSampler",R,V_frameSampler
audioSamples
T@"NSMutableArray",R,V_audioSamples
assetWriter
T@"HMIVideoAssetWriter",R,V_assetWriter
posterFrameGenerator
T@"HMICameraVideoPosterFrameGenerator",R,V_posterFrameGenerator
frameSelector
T@"HMICameraVideoFrameSelector",R,V_frameSelector
assetReader
T@"HMICameraVideoAssetReader",R,V_assetReader
T@"HMICameraVideoAnalyzer",R,V_analyzer
videoFrameResults
T@"NSMutableArray",&,V_videoFrameResults
significantEvents
shouldSkipAnalysis
shouldFailAnalysis
HMICameraVideoAnalyzerScheduler
HIGH
Normal
Warning
Critical
Undefined
analysisTime: %.2f (%.2f), total: %.2f (%.2f), level: %@, memory: %@, availableSystemMemory: %@, temp: %.2f, thermalLevel: %lu, analysisFPS: %.2f, analyzers: %lu, activeAnalyzers: %lu, maxAnalyzers: %lu
v16@?0@"HMICameraVideoAnalyzerRequest"8
[%@] [A:%d]
 %@ [%lu,%lu] %@
camera.video.analyzer.scheduler
internalAnalyzers
T@"NSPointerArray",R,V_internalAnalyzers
systemResourceUsageMonitor
T@"HMISystemResourceUsageMonitor",R,V_systemResourceUsageMonitor
systemResourceUsageMonitorUsageLevel
Tq,V_systemResourceUsageMonitorUsageLevel
averageAnalysisTime
averageAnalysisTimeMovingAverage
T@"MovingAverage",R,V_averageAnalysisTimeMovingAverage
averageTotalAnalysisTime
averageTotalAnalysisTimeMovingAverage
T@"MovingAverage",R,V_averageTotalAnalysisTimeMovingAverage
analysisFPSPreference
Td,R,V_analysisFPSPreference
paused
TB,GisPaused,V_paused
analyzers
maxConcurrentAnalyzers
TQ,R,V_maxConcurrentAnalyzers
activeAnalyzerCount
transcodingAnalyzerCount
HMICameraVideoAnalyzer
Disabling transcoding because too many transcoding analyzers were created.
%@Video fragment duration: %fs is greater than expected value: %fs
%@Video fragment sequence number: %lu is not equal to expected value: %lu
%@Video fragment has no frames
HMIHomePersonManager settings: %@
HMIHomePersonManager homeUUID: %@
HMIHomePersonManager is nil, face classification disabled
XPC connection was interrupted, retrying.
Unknown delegate name: %@
%@Failed to start reading of the asset: %@
v32@?0@"AVAsset"8@"HMICameraVideoResourceAttributes"16@"NSError"24
Failed to encode fragment.
Transcoded, bytes: %lu (%f), original bytes %lu 
End analysis: didAnalyze, significant events detected: %@
v16@?0@"HMICameraVideoFrameResult"8
End analysis, time spent: %fs, elapsed time since submission: %fs, predicted: %@
Stopping analysis due to high system resource usage
Stopping analysis due to cancelling
Stopping analysis due to entering full bypass mode
Stopping analysis due to analysis time past the maximum fragment analysis time: %fs
Finished early @ %@
Frame selector produced 0 frames
Analyzed frame:%lu, Events:%@
Failed saving activity zone data into file
[%@] <%lu> Object coordinate:%@
@"NSSet"16@?0@"HMIVideoAnalyzerEvent"8
outcome
cameraManufacturer
cameraModel
faceRecognitionEnabled
faceDetected
transcoded
activeAnalyzers
framesAnalyzedFragment
underlyingError
personScore
personConfidence
petScore
petConfidence
vehicleScore
vehicleConfidence
Uploading analytics event %@
com.apple.HomeAI.VideoAnalyzerStats
@"NSMutableDictionary"8@?0
com.apple.HomeAI
homeai: %@
camera.video.analyzer
internalPendingRequests
T@"NSMutableArray",R,V_internalPendingRequests
lastRequestSubmissionTime
T@"NSDate",&,V_lastRequestSubmissionTime
history
T@"HMICameraVideoAnalyzerHistory",R,V_history
streamAnalyzer
T@"HMIVideoAnalyzer",R,V_streamAnalyzer
currentRequest
T@"HMICameraVideoAnalyzerRequest",&,V_currentRequest
scheduler
T@"HMICameraVideoAnalyzerScheduler",R,V_scheduler
mediaIntegritySequenceNumber
TQ,V_mediaIntegritySequenceNumber
skipSequentialMediaIntegrityCheck
TB,R,V_skipSequentialMediaIntegrityCheck
analysisInProgress
TB,V_analysisInProgress
inErrorState
TB,V_inErrorState
inBypassMode
TB,V_inBypassMode
configuration
T@"HMICameraVideoAnalyzerConfiguration",&,V_configuration
remoteAnalysisService
T@"HMIAnalysisService",&,N,V_remoteAnalysisService
sessionEnded
TB,V_sessionEnded
uploadVideoAnalysisEvent
TB,R,GshouldUploadVideoAnalysisEvent,V_uploadVideoAnalysisEvent
saveVideoFramesToDisk
TB,GshouldSaveVideoFramesToDisk,V_saveVideoFramesToDisk
pendingRequests
isActive
TB,R,V_transcodeFragment
T@"<HMICameraVideoAnalyzerDelegate>",W,V_delegate
homePersonManager
T@"HMIHomePersonManager",&,V_homePersonManager
externalPersonManagers
T@"NSSet",&,V_externalPersonManagers
HMIMutableFloatArray
T@"NSData",R,&,N
floats
Tr^f,R,N
mutableFloats
T^f,R,N
Unable to get face quality prediction from the SVM model : %@
Face below face quality thresholds: SVM prediction = %lf, Yaw = %lf, discarding
Face with face mask detected, discarding
Face below ANFR face quality threshold: ANFR confidence = %lf, discarding
personsModelPredictions is empty
Face removed from unknown and uncertian bucket, SSD confidence = %f, entropy of laplacian = %f, Face yaw = %f, box size: %f
Added to unknown bucket yaw: %@
Added to uncertain bucket yaw: %@
@16@?0@"HMIPersonsModelPrediction"8
Face recognition set is empty
face.classifier.vip
faceprinter
T@"HMIFaceprinter",R,V_faceprinter
faceQualityFilter
T@"HMIFaceQualityFilterSVM",R,V_faceQualityFilter
classificationThresholdKnown
Td,R,V_classificationThresholdKnown
classificationThresholdUnknown
Td,R,V_classificationThresholdUnknown
Initializing HMICameraVideoFrameAnalyzerFactory
Resetting VideoFrameAnalyzer
Warm starting model...
warm_start_model
Failed to create pixel buffer when warm starting model
Failed to warm start model: %@
Warm start of model took: %f
Model is not bundled into framework
Analyze Frame
Failed handleMotionDetection with error %@
camera.video.frame.analyzer.factory
sharedInstance
T@"HMICameraVideoFrameAnalyzerFactory",R
frameAnalyzer
T@"<HMICameraVideoFrameAnalyzer>",&,N,V_frameAnalyzer
bounds
label
confidence
randomUniform
{CGAffineTransform=dddddd}
type
T@"NSString",R,V_name
T@"NSString",R,V_type
T@"HMICIFilterAttributeValue",R,V_value
probability
@16@?0@8
B16@?0@"HMICIFilterAttribute"8
HMIRotate
CIAffineTransform
inputAngle
inputTransform
v16@?0@"HMICIFilterAttribute"8
Td,R,V_probability
T@"NSArray",R,V_attributes
filters
v16@?0@8
com.apple.HomeAIDESPlugin
DES record saving is not permitted.
There isn't enough available disk space (%ld MB) to save DES records.
Couldn't determine the amount of available space disk space, continuing.
Saving DES Record, recordInfo: %@, data: %@
Saved DES Record: %@, error: %@
v24@?0@"NSUUID"8@"NSError"16
imageData
T@"NSData",R,V_imageData
createInputTensorWithError %@
HMIDESDataSet
samples
T@"NSArray",R,V_samples
imageName
T@"NSString",R,V_imageName
boxesName
T@"NSString",R,V_boxesName
weightsName
T@"NSString",R,V_weightsName
classesName
T@"NSString",R,V_classesName
bias_b
bias_g
bias_r
is_network_bgr
scale
thresholdWithLabels
T@"NSDictionary",R,V_thresholdWithLabels
metricWithLabels
T@"NSDictionary",R,V_metricWithLabels
thresholdDefault
T@"NSNumber",R,V_thresholdDefault
metricDefault
T@"NSNumber",R,V_metricDefault
Label:%d Confidence:%f X:%f Y:%f Width:%f Height:%f Yaw:%@}
labelIndex
Ti,R,V_labelIndex
Td,R,V_confidence
boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
q24@?0@"HMIObjectDetection"8@"HMIObjectDetection"16
@16@?0@"HMIObjectDetection"8
<%@: %p> timeStamp: %@, detections: [%@]
<%@: %p> %@
HMIVideoFrameAnalyzer
Analyzing %@
T@"<HMIVideoFrameAnalyzerDelegate>",W,V_delegate
sessionIdentifier
T@"NSUUID",R,V_sessionIdentifier
recognizeFaces
TB,V_recognizeFaces
frameAnalyzerDidAnalyzeFrame
T@?,C,V_frameAnalyzerDidAnalyzeFrame
timeOffset
T{?=qiIq},R,V_timeOffset
width
TQ,R,V_width
height
TQ,R,V_height
generationFrequency
T{?=qiIq},R,V_generationFrequency
frameHeight
TQ,R,V_frameHeight
[Fragment:%lu] Failed to generate poster frame for: %lu
camera.video.poster.frame.generator
posterFramesInternal
T@"NSMutableArray",&,V_posterFramesInternal
T@"HMICameraVideoPosterFrameGeneratorInput",R,V_input
nextGenerationTime
T{?=qiIq},V_nextGenerationTime
HMIFC.ck.u
HMIFC.ck.dr
HMIFC.ck.dc
HMIFC.ck.fbb
PVFC:PVFC
PVFC_FB
PVFC_CB
Ignoring error detecting face in Photos face crop, error: %@
Data Representation
Date Created
Face Bounding Box
Could not create image source
No meta data exists on image
Could not initialize from decoded UUID: %@ dataRepresentation: %@ dateCreated: %@
T@"NSUUID",R,C,V_UUID
T@"NSData",R,C,V_dataRepresentation
T@"NSDate",R,C,V_dateCreated
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_faceBoundingBox
HMIVideoRetimer
mediaType == kCMMediaType_Video
T@"<HMIVideoRetimerDelegate>",W,V_delegate
retimerDidRetimeSampleBuffer
T@?,C,V_retimerDidRetimeSampleBuffer
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
Descriptor count = 
Descriptor length = 
 bytes
 = [
HMICamera
Identifier
Name
Manufacturer
Model
T@"NSUUID",R,V_identifier
manufacturer
T@"NSString",R,V_manufacturer
model
T@"NSString",R,V_model
Descriptor vectors nil
Error
UpdatePersonsModelTask
RemovePersonsModelTask
HomePersonClusteringTask
TuriTrialUpdateTask
FaceMisclassificationTask
PersonsModelsSummaryTask
FeedbackTask
EmptyTask
taskType
sourceUUID
isExternal
cameraProfileUUID
clipUUID
taskID
Ti,R,V_taskID
results
T@"NSDictionary",R
T@"NSUUID",R,V_homeUUID
TaskID: %u running...
options is empty/nil, defaulting to Home persons clustering task
Unknown task type: %@
v32@?0@"HMITask"8Q16^B24
HMITaskHomeUUIDKey is nil
HMIUpdatePersonsModelTaskIsExternal is nil
HMIPersonsModelTaskSourceUUID is nil
Creating HMPhotosPersonManager for homeUUID:%@ sourceUUID:%@
Failed to get HMPhotosPersonManager
Creating HMHomePersonManager for homeUUID:%@
Failed to get HMHomePersonManager
Failed to initialize data source
Disk-based Home Person Data Source not supported
Current device is not primary resident, skipping clustering
HomeUUID is nil
task.service.server
nextTaskID
Ti,V_nextTaskID
Initializing HMITaskServiceServer
HMITaskService not available on this platform.
task.service
Error fetching persons:%@
fetch.persons.operation
shortDescription
privateDescription
propertyDescription
attributeDescriptions
T@"NSArray",R,C,N
T@"<HMIPersonManagerDataSource>",R,V_dataSource
persons
T@"NSSet",R,V_persons
%@ - %f
score
Tf,R,V_score
sparse
@"HMICameraVideoFrame"16@?0@"HMICameraVideoFrameSelectorFrameScore"8
q24@?0@"HMICameraVideoFrame"8@"HMICameraVideoFrame"16
q24@?0@"HMICameraVideoFrameSelectorFrameScore"8@"HMICameraVideoFrameSelectorFrameScore"16
v16@?0@"HMICameraVideoFrame"8
camera.video.frame.selector
sampler
T@"HMICameraVideoFrameSampler",R,V_sampler
sampleRate
T{?=qiIq},R,V_sampleRate
framesInternal
T@"NSMutableArray",R,V_framesInternal
maxFrameCount
Tq,R,V_maxFrameCount
predictedFrames
T@"NSMutableArray",R,V_predictedFrames
detector
T@"<HMIMotionDetector>",R,V_detector
T@"<HMICameraVideoFrameSelectorDelegate>",W,V_delegate
frames
camera.video.frame.sampler
targetInterval
T{?=qiIq},R,V_targetInterval
sampleInterval
T{?=qiIq},R,V_sampleInterval
unmatchedSampleFrames
T@"NSMutableArray",R,V_unmatchedSampleFrames
T@"HMICameraVideoFrame",&,V_frame
markedAsFinished
TB,GisMarkedAsFinished,V_markedAsFinished
T@"<HMICameraVideoFrameSamplerDelegate>",W,V_delegate
allAtCurrentVersion
existingAtOtherVersions
T@"NSSet",R,V_existingAtOtherVersions
createdAtCurrentVersion
T@"NSSet",R,V_createdAtCurrentVersion
existingAtCurrentVersion
T@"NSSet",R,V_existingAtCurrentVersion
Vision
Faceprint Version: %ld.%ld
Warm starting faceprint model...
warm_start_faceprint_model
Failed to create pixel buffer when warm starting faceprint model
Failed to warm start faceprint model: %@
Warm start of faceprint model took: %f
CreateFaceprint
Cropping face %@ from pixel buffer with dimensions: %.1f x %.1f roll: %.02f degrees
Error pixel buffer type conversion.
Error in rotating the face.
Face was rotated by:%.02f degrees
Cropping face %@ from face crop with dimensions %.1f x %.1f
B16@?0@"HMIFaceprint"8
%lu faceprint(s) exist for face crop:%@ but are not the current version
Using existing faceprint for face crop:%@
Faceprinting face crop:%@
Skipping crop, encountered error faceprinting: %@
Vision run-time version: %d.%02d.%02d (%d)
face.detector.vision
HMIFP.ck.u
HMIFP.ck.d
HMIFP.ck.mu
HMIFP.ck.fcu
modelUUID
faceCropUUID
Data
Model UUID
Face Crop UUID
Could not initialize from decoded UUID: %@ data: %@ modelUUID: %@ faceCropUUID: %@
T@"NSData",R,C,V_data
T@"NSUUID",R,C,V_modelUUID
T@"NSUUID",R,C,V_faceCropUUID
<%@ %@>
point
T{CGPoint=dd},R,V_point
HMPMS.fce
Face Classification Enabled
TB,GisFaceClassificationEnabled,V_faceClassificationEnabled
TB,D,GisFaceClassificationEnabled
com.apple.homeai.model.loader
/var/mobile/Library/Caches/com.apple.HomeAI/model_dir
[Model loading] failed to create asset dir: %@ error: %@
[Model loading] unpackaging
[Model loading] failed to remove dir
[Model loading] failed to remove dir %@ err %@
[Model loading] failed to untar model
[Model loading] failed to untar model asset into %@ err %@
HMIModelLoader
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
pendingUpdates
T@"NSMutableSet",&,N,V_pendingUpdates
clip
faceCrops
authTag
internal
wrappedKey
1.2.840.113635.100.6.27.38
HMIFeedback Queue
HMIFeedback
session
T@"NSURLSession",R,V_session
feedbackServiceHost
T@"NSString",R,V_feedbackServiceHost
homeKitClient
T@"HMIHomeKitClient",R,V_homeKitClient
Trusting host: %@
Force Certificate Pinning
HFFeedbackService
Error setting trust policies: %lu
Invalid certificate: %@
Downloading Clip
Cannot find camera profile.
Cannot find home for camera profile.
Fetched Clip videoAssetContext: %@, error: %@
Cannot download clip asset.
Check network connectivity.
v24@?0@"HMCameraClipVideoAssetContext"8@"NSError"16
Fetching Clip, progress %lu%%
v16@?0Q8
v16@?0@"HMCameraClip"8
Cannot download clip for camera profile.
Ensure the clip belongs to the camera profile.
Face crops are not available.
Fetching Face Crops
@16@?0@"HMCameraClipSignificantEvent"8
Fetched Person UUIDs: %@
Fetched Face Crops: %@, error: %@
v24@?0@"HMCameraClip"8@"NSError"16
Fetching Clip
Clip doesn't have a video track.
%@%@
hkcvml-dev.apple.com
hkcvml.apple.com
https://%@/v2/clip-uuid/
feedback
%@.%@
Uploading payload data: %@, to URL %@
json
application/json
Content-Type
v32@?0@"NSData"8@"NSURLResponse"16@"NSError"24
Invalid key size.
Cannot generate initialization vector.
Failed to encrypt data.
Failed to encrypt face crop data.
Submitting clipUUID: %@, cameraProfileUUID: %@
Stripped Audio %@, error: %@
v24@?0@"NSURL"8@"NSError"16
Downloaded %@, error: %@
Failed to fetch pre-signed URL, error: %@
Failed to request service result, error: %@
Failed to decode service result, error: %@
Service result: %@
Deleting Temporary File %@
Deleted Temporary File %@, error: %@
v16@?0@"NSURL"8
HMIFeedbackSubmitClipOperation
feedbackSession
T@"HMIFeedbackSession",R,V_feedbackSession
T@"NSUUID",R,V_cameraProfileUUID
T@"NSUUID",R,V_clipUUID
temporaryFileURLs
T@"NSMutableArray",R,V_temporaryFileURLs
T@"NSSet",&,V_faceCrops
assetData
T@"NSData",&,V_assetData
serviceResult
T@"NSDictionary",&,V_serviceResult
HMCameraClipFetchVideoAssetContextOperation
HMIFC.pu
HMIFC.su
HMIFC.seu
HMIFC.fc
HMIFC.fp
HMIFC.c
HMIFC.f
Known
Uncertain
Source UUID
Session Entity UUID
Familiarity
FaceCrop UUID
Faceprint UUID
T@"NSString",R,V_identifier
personsModelIdentifier
T@"NSString",R,V_personsModelIdentifier
T@"NSUUID",R
T@"NSUUID",R,V_personUUID
T@"NSUUID",R,V_sourceUUID
sessionEntityUUID
T@"NSUUID",R,V_sessionEntityUUID
familiarity
Tq,R,V_familiarity
Failed to generate persons model summmary, error:%@
persons.models.summary.task
Failed to fetch face crops with error: %@
@16@?0@"HMIPersonFaceCrop"8
Failed to fetch faceprints with error: %@
Error faceprinting face crops:%@
Person (%@) has no faceprints -- nothing to remove
Nearest face crop to be removed: %@
Failed to remove face crop with error: %@
Successfully removed face crop (%@) via user indicated misclassification
face.misclassification.task
T@"HMIPersonFaceCrop",R,V_faceCrop
Failed to remove persons model, error:%@
Successfully removed persons model
SourceUUID:%@ HomeUUID:%@
remove.persons.model.task
T@"NSUUID",R,C,V_homeUUID
T@"<HMIPersonManagerDataSource>",W,N,V_dataSource
supportsFaceClassification
TB,V_supportsFaceClassification
personDataAvailableViaHomeKit
TB,GisPersonDataAvailableViaHomeKit,V_personDataAvailableViaHomeKit
%.4f
%.2f[%c]
levelThresholds
T@"NSArray",R,V_levelThresholds
Td,R,V_value
HMIErrorDomain
Unexpected error
Failed to analyze
Failed to read fragment
Failed to verify fragment
Failed to generate poster frame
Failed to transcode fragment
Video analyzer in error state
Model failed to load
Fragment is invalid
No pixel buffer
Unknown error code %ld
HMIErrorCodeUnexpectedError
HMIErrorCodeFailedToAnalyze
HMIErrorCodeFailedToReadFragment
HMIErrorCodeFailedToVerifyFragment
HMIErrorCodeFailedToGeneratePosterFrame
HMIErrorcodeFailedToTranscodeFragment
HMIErrorCodeAnalyzerInErrorState
HMIErrorCodeModelFailedToLoad
HMIErrorCodeFragmentIsInvalid
HMIPrivateErrorCodeNilPixelBuffer
HMIPrivateErrorCodeEmptyURL
HMIPrivateErrorCodeAVAssetReaderInitializationFailed
HMIPrivateErrorCodeFailedToLoadProperty
HMIPrivateErrorCodeAssetLoadCancelled
HMIPrivateErrorCodeReadingStartFailed
HMIPrivateErrorCodeNoTrackOutput
HMIPrivateErrorCodeSampleBufferUnavailable
HMIPrivateErrorCodeNoVideoTrackFound
HMIPrivateErrorCodeMultipleVideoTracksFound
HMIPrivateErrorInvalidPresentationTime
HMIPrivateErrorAVAssetReaderNotStarted
HMIPrivateErrorCodeSequentialIntegrityViolated
HMIPrivateErrorCodeAnalysisServiceNoConfiguration
HMIPrivateErrorCodeLoadingCoreMLModelFailed
HMIPrivateErrorCodeCoreMLPredictionFailed
HMIPrivateErrorCodeCoreMLOutputIncorrect
HMIPrivateErrorCodeCropAndResizeFailed
HMIPrivateErrorCodePosterFrameGenerationFailed
HMIPrivateErrorCodeCodecNotAvailable
HMIPrivateErrorCodeEncodingFailed
HMIPrivateErrorCodeInvalidVideoFrameFormatToSave
HMIPrivateErrorCodeScalerError
HMIPrivateErrorCodeFaceprintingFailed
HMIPrivateErrorCodeUpdatePersonsModelTaskFailed
HMIPrivateErrorCodeExternalPersonSourceDiskError
HMIPrivateErrorCodeLoadPersonsModelsFailed
HMIPrivateErrorCodeUpdatePersonsModelFailed
HMIPrivateErrorCodeRemovePersonsModelFailed
HMIPrivateErrorCodePersonsModelPredictionFailed
HMIPrivateErrorCodeNilDataSource
HMIPrivateErrorCodeUnknownTask
HMIPrivateErrorCodeHomePersonClusteringTaskFailed
HMIPrivateErrorCodeRemovePersonsModelTaskFailed
HMIPrivateErrorCodeFaceMisclassificationTaskFailed
ERROR_%ld
%@: %@
Provided sequenceNumbers: %@, don't match fragment's sequenceNumbers: %@
v12@?0I8
B28@?0I8@"NSData"12@"NSData"20
video/mp4
Failed to read fragment data, err: %d
first
second
HMIVideoFragment
[%@]
Initialization Segment Data
Separable Segment Data
Sequence Numbers
Time Range
T@"NSData",R,C
videoFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_videoFormatDescription
audioFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_audioFormatDescription
videoTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_videoTrackTimeRange
audioTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_audioTrackTimeRange
initializationSegment
T@"NSData",R,V_initializationSegment
separableSegment
T@"NSData",R,V_separableSegment
timeRange
T{?={?=qiIq}{?=qiIq}},R,V_timeRange
sequenceNumbers
T@"NSArray",R,V_sequenceNumbers
HMISessionEntityManager
WARNING: received multiple motion detections -- using first
WARNING: no motion detection to use for face tracking
Face event doesn't have a recognition, ignoring
B16@?0@"NSUUID"8
v24@?0@"NSUUID"8^B16
v16@?0@"HMIFaceprint"8
Transition Pr(detectionIdx: %lu, previousIndex: %lu, sessionUUID:%@) = %lf
Face embedding distance to cluster (detectionIdx: %lu, sessionUUID:%@) = %lf
Face embedding distance to previous face (detectionIdx: %lu, sessionUUID:%@) = %lf
Dynamic threshold (detectionIdx: %lu, sessionUUID:%@) = %lf
Adding new sessionEntityUUID: %@
@16@?0@"HMIFaceClassification"8
v32@?0@"HMIVideoAnalyzerEventPerson"8Q16^B24
sessionEntities
T@"NSMutableDictionary",R,V_sessionEntities
HMITrialUpdateTask
v16@?0@"HMIPoint"8
HMIVideoAnnotator
resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorProtocol>",R,V_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorDelegate>",W,Vdelegate
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPSUP.ck.p
HMIPSUP.ck.s
Could not initialize from decoded sourceUUID: %@
T@"NSUUID",R,C,V_sourceUUID
HMIP.ck.u
HMIP.ck.n
HMIP.ck.pl
personLinks
@"HMIPersonSourceUUIDPair"16@?0@"HMPersonLink"8
Could not initialize from decoded UUID: %@
T@"NSSet",R,C,V_personLinks
externalLibrary
TB,R,GisExternalLibrary,V_externalLibrary
faceCountsByPerson
T@"NSDictionary",R,V_faceCountsByPerson
v16@?0@"<NSObject><NSCopying><NSSecureCoding>"8
HMIPersonsModel
visionPersonsModel
T@"VNPersonsModel",R,V_visionPersonsModel
summary
T@"HMIPersonsModelSummary",R
data:;base64,%@
@24@?0@8@16
%.6f
options
Tq,V_options
objectJSON
T@"NSString",R
objectPrettyJSON
data:;base64,
Invalid JSONObject class name: %@
Invalid JSONObject: %@
Cannot parse JSON data: %@
container
T@,R,V_container
classMap
T@"NSDictionary",&,V_classMap
yyyy-MM-dd'T'HH:mm:ss.SSS'Z'
Error creating directory:%@ to save face classifications
faceprintUUID
%@-%03lu-%03u-%@-%@-%@
known
unknown
Error saving metadata to disk for face classification:%@
Error saving face crop image to disk for face classification:%@
Saved face classification:%@ to disk
face.utilities
frameDimensions.width > 0 && frameDimensions.height > 0
v32@?0@"HMISparseOpticalFlowFeatureVector"8Q16^B24
HMIPersonBlob (%@): {center = (%f, %f),  size = (%f, %f), source = %@}
detection
projection
frameDimensions
T{CGSize=dd},R,V_frameDimensions
T{CGRect={CGPoint=dd}{CGSize=dd}},V_faceBoundingBox
personBoundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_personBoundingBox
position
T{CGPoint=dd},V_position
motionMean
T{CGVector=dd},V_motionMean
personIndex
T@"NSNumber",&,V_personIndex
personIndices
T@"NSMutableSet",&,V_personIndices
personIouMax
Tf,V_personIouMax
blobID
T@"NSUUID",R,V_blobID
projectedFaceIndex
TQ,R,V_projectedFaceIndex
detectedFaceIndex
TQ,R,V_detectedFaceIndex
Td,R,V_score
q24@?0@"HMIFaceTrackerMatch"8@"HMIFaceTrackerMatch"16
motionVectors
T@"NSArray",R,V_motionVectors
q24@?0@"HMIFaceTrackerMotionRecord"8@"HMIFaceTrackerMotionRecord"16
HMIFaceTracker
Handling motion detection update for sessionPTS: %f
@"NSArray"16@?0@"HMISparseOpticalFlowMotionDetection"8
newPersonWithFaceEvents.count > 0 || personWithoutFaceEvents.count > 0 || personWithFilteredFaceEvents.count > 0
@"HMIPersonBlob"16@?0@"HMIVideoAnalyzerEventPerson"8
Initial faces consumed
Timeout reached -- resetting face tracker
@"HMIPersonBlob"16@?0@"HMIPersonBlob"8
v32@?0@"HMIPersonBlob"8Q16^B24
v16@?0@"HMIFaceTrackerMatch"8
v32@?0@"NSNumber"8@"NSNumber"16^B24
B16@?0@"HMIPersonBlob"8
v24@?0Q8^B16
v32@?0@"HMIFaceTrackerMatch"8Q16^B24
v32@?0@"NSUUID"8@"NSNumber"16^B24
v32@?0@"NSNumber"8Q16^B24
previousPersons
T@"NSArray",&,V_previousPersons
previousProjectedPersonIndices
T@"NSArray",&,V_previousProjectedPersonIndices
previousFilteredPersonIndices
T@"NSArray",&,V_previousFilteredPersonIndices
motionRecordsQueue
T@"NSMutableArray",R,V_motionRecordsQueue
previousTime
T{?=qiIq},V_previousTime
Failed to initialize HMICameraVideoResourceAttributes from fragment data, err: %d
camera.video.resource.attributes
assetDuration
T{?=qiIq},R,V_assetDuration
T@"NSDate",R,V_creationDate
firstSequenceNumber
TQ,R,V_firstSequenceNumber
nominalFrameRate
Td,R,V_nominalFrameRate
dimensions
T{CGSize=dd},R,V_dimensions
baseDecodeTimeStamp
T{?=qiIq},R,V_baseDecodeTimeStamp
firstFragmentSequenceNumber
fragmentCount
formatDescriptions
[Fragment:%lu] Creating asset with url: %@
[Fragment:%lu] Failed to initialize AVAssetReader
[Fragment:%lu] Start %0.2f, Duration %0.2f, FPS %0.2f
[Fragment:%lu] No fragment URL found on loading request %@
[Fragment:%lu] No fragment data found for URL %@
[Fragment:%lu] Finished handling load request
camera.video.asset.reader
currentFrameId
TQ,V_currentFrameId
T@"AVAssetReader",&,N,V_assetReader
resourceLoaderWorkQueue
T@"NSObject<OS_dispatch_queue>",R,V_resourceLoaderWorkQueue
T@"HMICameraVideoFragment",R,V_videoFragment
logIdentifier
T@"NSString",R,V_logIdentifier
com.apple.videotoolbox.videoencoder.h264.rtvc
Error in VTCompressionSessionEncodeFrameWithOutputHandler %d
Frame dropped.
v24@?0i8I12^{opaqueCMSampleBuffer=}16
Error Calling VTCompressionSessionEncodeFrameWithOutputHandler, error: %d
Attempting recovery.
Recovered successfuly.
Failed to encode sample after resetting encoder session.
Failed to recover, error %@
Video encoder is not running, ignoring %@
Failed to encode and recover from failure.
HMIVideoEncoder
T@"<HMIVideoEncoderDelegate>",W,V_delegate
averageBitRate
maxKeyFrameIntervalDuration
numberOfDroppedFrames
TQ,R,V_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
T@?,C,V_encoderDidEncodeSampleBuffer
encoderDidFailWithError
T@?,C,V_encoderDidFailWithError
Cannot set property, %d
addFaceCrops:%@
@16@?0@"HMIFaceCrop"8
addPersonFaceCrops:%@
addPersons:%@
@16@?0@"HMIPerson"8
fetchAllUnassociatedFaceCropsWithCompletion
@16@?0@"HMFaceCrop"8
removeFaceCropsWithUUIDs:%@
removePersonsWithUUIDs:%@
associateFaceCropsWithUUIDs:%@ toPersonWithUUID:%@
home.person.datasource.homekit
T@"HMHomePersonManager",&,V_homePersonManager
HMFaceCrop
HMPersonFaceCrop
HMMutablePerson
HMPMS.ifple
HMPMS.sfce
Importing From Photo Library Enabled
Sharing Face Classifications Enabled
importingFromPhotoLibraryEnabled
TB,GisImportingFromPhotoLibraryEnabled,V_importingFromPhotoLibraryEnabled
sharingFaceClassificationsEnabled
TB,GisSharingFaceClassificationsEnabled,V_sharingFaceClassificationsEnabled
TB,D,GisImportingFromPhotoLibraryEnabled
TB,D,GisSharingFaceClassificationsEnabled
detectionScore
faceFilteredState
homeFamiliarity
v16@?0@"HMIFaceClassification"8
q24@?0@"NSNumber"8@"NSNumber"16
externalFamiliarity
sessionEntityAssignment
com.apple.HomeAI.FaceEvent
faceprintsClustered
clusters
personsCreated
faceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
errorCode
errorDescription
com.apple.HomeAI.FaceClustering
v24@?0@"NSString"8@"NSNumber"16
v16@?0@"HMIPersonsModelSummary"8
@avg.self
externalLibraries
homeIdentities
averageExternalIdentities
averageHomeFaceCrops
averageExternalFaceCrops
homeToExternal
externalToExternal
com.apple.HomeAI.PersonsModels
@"NSDictionary"8@?0
B16@?0@"NSNumber"8
HMIAnalytics
personsmodels
home
external
user_defined_person_links.bin
modelSummaries
T@"NSSet",R,V_modelSummaries
homeToExternalEquivalencies
TQ,R,V_homeToExternalEquivalencies
externalToExternalEquivalencies
TQ,R,V_externalToExternalEquivalencies
T@"NSNumber",R,V_confidence
linkedEntityUUID
T@"NSUUID",R,V_linkedEntityUUID
v32@?0@"NSUUID"8@"HMIPersonsModel"16^B24
v16@?0@"HMIPerson"8
Writing updated userDefinedPersonLinksByHome[%@] to disk
Error adding faceprints to model for personUUID: %@
v32@?0@"HMIPerson"8@"NSSet"16^B24
Stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@ detected, attempting to remove...
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@, error getting model storage path
Stale model path no longer on disk, proceeding with building persons model...
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Persisted VNPersonsModel for homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Did not remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, no model found
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Error removing user defined person links file: %@
Removed userDefinedPersonLinksByHome for homeUUID: %@
Removed VNPersonsModel for homeUUID: %@ sourceUUID:%@
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Home persons model not found for homeUUID: %@
Failed to predict using VNPersonsModel for home persons model
Unable to build equivalency map for homeUUID: %@, error: %@
Unable to get person model for homeUUID: %@ (self.personsModelsByHome is %@ nil)
Failed to predict using VNPersonsModel for homeUUID: %@ sourceUUID: %@
Failure to lookup equivalency cell for %@ (equivalencyCellForHome is%@ nil)
 not
v24@?0@"HMIPersonSourceUUIDPair"8^B16
v32@?0@"NSSet"8@"HMIPersonsModelPrediction"16^B24
Resetting HMIPersonsModelManager
%@.bin
Directory to load/store persons models (%@) does not exist
Failed to enumerate persons models at path (%@)
\A[A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12}\.bin\Z
B16@?0@"NSURL"8
Persons Model Storage Path:%@
Failed to enumerate homes at path: %@
Failed to parse Home UUID from path: %@
Failed to load External HMIPersonsModel at path: %@, error: %@
Loaded External HMIPersonsModel for homeUUID: %@ sourceUUID: %@
Directory to load/store home persons model (%@) contains multiples files: %@ there can only be one
No home model found for homeUUID: %@
Failed to load Home HMIPersonsModel at path: %@, error: %@
Loaded Home HMIPersonsModel for homeUUID: %@ sourceUUID: %@
Loaded %lu user defined equivalencies found for home: %@
No user defined equivalencies found for home: %@ (reason: %@)
Invalid file path in load model attempt: %@
Failed to load VNPersonsModel at path: %@
@24@?0@"NSUUID"8@"HMIPersonsModel"16
v32@?0@"HMIPersonSourceUUIDPair"8@"NSSet"16^B24
persons.model.manager
T@"HMIPersonsModelManager",R
userDefinedPersonLinksByHome
T@"NSDictionary",R,V_userDefinedPersonLinksByHome
personsModelsByHome
T@"NSDictionary",R,V_personsModelsByHome
equivalencyTablesByHome
T@"NSDictionary",R,V_equivalencyTablesByHome
weights
T@"HMIDESMutableFloatArray",R,V_weights
biases
T@"HMIDESMutableFloatArray",R,V_biases
layerParameters
T@"NSArray",R,V_layerParameters
losses
T@"NSArray",R,V_losses
HMIDESTrainer
is_training
checkpoint
Loss/total_loss
trainingCallback %lu, loss: %f
B32@?0Q8@"<ETDataProvider>"16@"<ETTaskContext>"24
Training was skipped because %@ is YES.
Training Started
Training Finished
v16@?0@"HMIDESLayerParameters"8
T@"HMIDESDataset",R,V_data
networkPath
T@"NSURL",R,V_networkPath
Unknown reason.
Skipped
{code: %@, message: "%@"}
{code: %@}
success
T@"HMIVideoAnalyzerResultOutcome",R
skipped
code
TQ,R,V_code
isSkipped
isSuccess
message
T@"NSString",R,V_message
v24@?0@"HMIExternalPersonManagerSettings"8@"NSError"16
Timer fired, updating external persons model
external.person.manager
Settings have disabled face classification, removing external persons model
Settings have enabled face classification, updating external persons model
T@"HMIExternalPersonManagerSettings",R,V_settings
T@"<HMIExternalPersonManagerDataSource>",W,N,V_dataSource
Failed to remove persons model, error:%@, retrying...
Submitted persons model remove task, taskID:%u, retryOnError:%@
remove.persons.model.operation
TB,R,V_external
HMIVideoAnalyzerScheduler
v32@?0@"HMIVideoAnalyzer"8Q16^B24
Scheduler state: 
usage: %@
, mem: %@
, max: %@
v16@?0@"HMIVideoAnalyzer"8
SignificantActivity.mlmodelc
HOMEAI_SIGNIFICANT_ACTIVITY_DETECTOR
v16@?0@"<TRINamespaceUpdateProtocol>"8
Submitted task returned error: %@
compiledModel
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
HMITuriTrialManager
Td,R,V_personThresholdHigh
Td,R,V_personThresholdMedium
Td,R,V_petThresholdHigh
Td,R,V_petThresholdMedium
Td,R,V_vehicleThresholdHigh
Td,R,V_vehicleThresholdMedium
Td,R,V_faceThreshold
modelPath
T@"NSString",R,V_modelPath
Clustering successful
Clustering error
greedy_clustering
descriptorA.length == descriptorB.length
cluster.objects.count > 0
faceObservations.count > 0
faceObservations.firstObject.faceprint != nullptr
faceprintLength == faceObservation.faceprint.lengthInBytes / sizeof(float)
HMIVideoAnalyzerLegacy
Analyzing fragment: %@, configuration: %@
Motion was assumed because AnyMotion was turned on.
Fragment analysis was skipped.
v16@?0@"HMIVideoAnalyzerFrameResult"8
@16@?0@"HMICameraVideoPosterFrame"8
Analyzed fragment result %@, with configuration: %@
legacyAnalyzer
T@"HMICameraVideoAnalyzer",R,V_legacyAnalyzer
HMIVideoFrameSampler
T@"<HMIVideoFrameSamplerDelegate>",W,V_delegate
HMIVideoFrameIntervalSampler
Sample buffer has an invalid PTS.
frameSamplerDidSampleFrame
T@?,C,V_frameSamplerDidSampleFrame
frameSamplerDidDropFrame
T@?,C,V_frameSamplerDidDropFrame
HMIVideoPackageAnalyzer
Detecting Package
T@"<HMIVideoPackageAnalyzerDelegate>",W,V_delegate
packageAnalyzerDidDetectPackage
T@?,C,V_packageAnalyzerDidDetectPackage
laplacian
detectionConfidence
boxSize
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
d24@?0@"NSString"8d16
face.qualityscore.consolidated
T@"NSDictionary",R,V_weights
cutOffThresholds
T@"NSDictionary",R,V_cutOffThresholds
maximumDifferences
T@"NSDictionary",R,V_maximumDifferences
HMIVideoAnalyzerFragmentResult
@"NSArray"16@?0@"HMIVideoAnalyzerFrameResult"8
{maxConfidenceEvents: [%@] frameResults: [%@], thumbnails: %@, fragment: %@}
T@"HMIVideoFragment",R,V_fragment
thumbnails
T@"NSArray",R,V_thumbnails
T@"HMIVideoAnalyzerResultOutcome",R,V_outcome
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_configuration
HMICVFR.f
HMICVFR.roi
HMICVFR.ae
HMICVAR.rc
HMICVAR.fr
HMICVAR.cd
HMICVAR.d
HMICVAR.pf
HMICVAR.ttaf
HMICVAR.tsfws
HMICVAR.lsn
HMICVAR.vf
HMICVAR.af
HMICVAC.pfgi
HMICVAC.pfh
HMICVAC.mfad
HMICVAC.mfd
HMICVAC.st
HMICVAC.smisn
HMICVAC.tf
HMICVAC.fce
HMICVAC.csd
HMICVAC.c
HMICVAC.hu
HMICVASE.e
HMICVASE.vf
HMICVASE.as
HMICVF.et
HMICVF.sn
HMICVF.mf
HMICVF.az
HMICVF.d
HMICVPF.d
HMICVPF.to
HMICVPF.w
HMICVPF.h
HMIOD.l
HMIOD.c
HMIOD.b.x
HMIOD.b.y
HMIOD.b.w
HMIOD.b.h
HMIOD.y.a
HMIOD.r.o
HMIP.x
HMIP.y
HMIAZ.p
HMIAZ.i
HMICameraVideoFrame does not support NSSecureCoding in the simulator
 isInclusion:%d 
class-label
coordinates
overlap
inclusion
activityZone
%@-%@-%@-%@.json
%@/%@
Error creating activity zone result directory: %@
%@/activityzone-%@
Activity zone file path:%@
Error converting activity zone results to JSON: %@
Error writing activity zone results JSON to file: %@
motionScore %f
B16@?0@"HMICameraActivityZone"8
Activity zones coordinates:%@
Inclusion zone:%@ intersecting with:(%@) Object coordinate %@
Exclusion zone:%@ intersecting with:(%@) Object coordinate %@
Events after activity zone filtering:(%@) Object coordinate %@
zoneType
filteringLevel
numDetectedObjects
com.apple.HomeAI.ActivityZone
noFiltering
resize_0
resize_10
resize_20
exclusion
resize_26
resize_36
camera.activity.zone
points
T@"NSArray",R,C,V_points
TB,R,GisInclusion,V_inclusion
HMIVideoAnalyzerConfiguration
Thumbnail Interval
Thumbnail Height
Max Fragment Duration
Max Fragment Analysis Duration
Transcode
Min Frame Quality
Min Frame Scale
Camera
Recognize Faces
minFrameQuality > 0 && minFrameQuality <= 1
minFrameScale > 0 && minFrameScale <= 1
thumbnailInterval
T{?=qiIq},V_thumbnailInterval
thumbnailHeight
TQ,V_thumbnailHeight
Td,V_maxFragmentAnalysisDuration
T{?=qiIq},V_maxFragmentDuration
transcode
TB,V_transcode
T@"HMICamera",&,V_camera
minFrameQuality
Td,V_minFrameQuality
minFrameScale
Td,V_minFrameScale
Event Triggers
Detect Packages
Activity Zone Count
HMIVideoAnalyzerDynamicConfiguration
detectPackages
TB,V_detectPackages
eventTriggers
Tq,V_eventTriggers
T@"NSArray",&,V_activityZones
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_boundingBox
Unsupported aspect ratio: (%d, %d)
preferenceOverrides
videoAnalyzerIdentifier
didAnalyzeFragment
didNotAnalyzeFragment
didFindSignificantEvent
result
HMIAnalysisServiceTypeUnknown
HMIAnalysisServiceTypeLocal
HMIAnalysisServiceTypeRemoteFragment
HMIAnalysisServiceTypeUndefined
v24@?0@"HMICameraVideoFragment"8@"HMICameraVideoAnalyzerResult"16
v24@?0@"HMICameraVideoFragment"8@"NSError"16
T@?,C,N,V_didAnalyzeFragment
didFailAnalysisForFragment
T@?,C,N,V_didFailAnalysisForFragment
T@?,C,N,V_didFindSignificantEvent
T@?,C,N,V_didNotAnalyzeFragment
HMIAnalysisService
Remote analysis not supported in the simulator
Asset data is ignored if the video fragment is passed through the properties dictionary
Analyzer configuration property key (%@) is missing
v24@?0@"HMICameraVideoAnalyzerSignificantEvent"8@"HMICameraVideoFragment"16
camera.video.analysis.service
nextRequestID
Ti,V_nextRequestID
requests
T@"NSMapTable",R,V_requests
runRemotely
TB,V_runRemotely
HMIVAEP.f
HMIVAEP.ibbe
Is Bounding Box & Confidence Estimated
Is Object Moving
Face
%@ %@
HMIVideoAnalyzerEventPerson
isBoundingBoxEstimated
TB,R,GhasEstimatedBoundingBox,V_isBoundingBoxEstimated
boundingBoxForFaceTracker
T{CGRect={CGPoint=dd}{CGSize=dd}},R
face
T@"HMIVideoAnalyzerEventFace",R,V_face
com.apple.homeai.pfl
PFLPrivatize Failed
Privatized Training Result
Encrypted Training Result
PFLPrivatizeCustomNorm
/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/Contents/MacOS/PrivateFederatedLearning
com.apple.homed
syntheticEvents
personDetected
petDetected
vehicleDetected
analysisQOS
analysisServiceType
motionDetector
frameSelectorSampleRateMultiplier
frameSelectorTargetInterval
processingDevice
confidenceThresholdPersonHigh
confidenceThresholdPetHigh
confidenceThresholdVehicleHigh
confidenceThresholdFaceHigh
confidenceThresholdPersonMedium
confidenceThresholdPetMedium
confidenceThresholdVehicleMedium
confidenceThresholdFaceMedium
modelTimeout
assetReaderMaximizePowerEfficiency
assetReaderForceBGRA
analysisEnableTranscodeFragment
realTimeEncoding
maxConcurrentAnalysisRequests
espressoLowPriority
opticalFlowLowPriority
opticalFlowBackgroundProcessing
enableAnalyzerHistory
saveDESRecords
DESSkipTraining
DESSkipTrainingScalar
DESSkipPrivatize
confidenceThresholdFace
personsModelStoragePath
personDataSourceDiskStoragePath
personsModelClassificationThresholdKnown
personsModelClassificationThresholdUnknown
saveFaceCropsToDisk
enableSavingActivityZoneInfo
annotateVideo
showROI
useDevelopmentFeedbackService
user-interactive
user-initiated
unspecified
default
utility
background
Preference %@ is now %@, previously was %@
Only NSNumber and NSString properties are supported.
HMIPreference
T@"HMIPreference",R
qosMap
isProductTypeJ42
isProductTypeJ105
isProductTypeB238
isAudioAccessory
isInternalInstall
preferenceCacheFlushTimer
T@"HMFTimer",R,V_preferenceCacheFlushTimer
preferenceCache
T@"NSMutableDictionary",R,N,V_preferenceCache
preferenceLoggedValues
T@"NSMutableDictionary",R,N,V_preferenceLoggedValues
preferenceOverridesInternal
T@"NSMutableDictionary",R,N,V_preferenceOverridesInternal
usesCPUOnly
maxVideoEncoderFrameRate
home.person.datasource.disk
Error fetching face crops for person:%@, error:%@
fetch.personfacecrops.operation
person
T@"HMIPerson",R,V_person
personFaceCrops
T@"NSSet",R,V_personFaceCrops
Already started listening for the notification
HMINotifydObserver
T@"NSObject<OS_dispatch_queue>",R,N,V_queue
callback
T@?,R,N,V_callback
token
Ti,N,V_token
notificationName
Tr*,R,N,V_notificationName
HMIVideoAnalyzerEventVehicle
com.apple.HomeAI.%@%@%@.%tu
Faceprints
Clusters
Persons
Associated
Faceprinting Duration
Clustering Duration
Total Duration
numberOfFaceprintsClustered
Tq,V_numberOfFaceprintsClustered
numberOfClusters
Tq,V_numberOfClusters
numberOfPersonsCreated
Tq,V_numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
Tq,V_numberOfUnknownFaceprintsAssociated
Td,V_faceprintingDuration
Td,V_clusteringDuration
Td,V_totalDuration
T@"NSError",&,V_error
HMIHomePersonClusteringTask
Error performing cloud pull:%@
Error fetching face crops:%@
@"NSUUID"16@?0@"HMIFaceCrop"8
Error fetching faceprints:%@
Storing newly created faceprints: %@
Error saving new faceprints:%@
Number of faceprints to cluster: %lu
Clustering error:%@
Number of clusters: %lu
Cluster of size %lu beneath threshold of %d
Face prediction error:%@
Error adding new persons:%@
@16@?0@"NSNumber"8
Error associating face crops with person (%@): %@
v32@?0@"VNCluster"8@"NSUUID"16^B24
Finished calls to associateFaceCropsWithUUIDs
Error associating face crops for %lu person%@: (
 ...
Fetching persons
@"NSUUID"16@?0@"HMIPerson"8
Fetched %lu persons (%lu unnamed)
Exiting early because task was canceled.
Skipping named person
B16@?0@"HMIPersonSourceUUIDPair"8
Fetching face crops for person: %@
Found unnamed person %@ with no face crops!
Finished calls to fetchFaceCropsForPersonsWithUUIDs
Error fetching removing persons with UUIDs:%@, error:%@
v16@?0@"HMIPersonFaceCrop"8
T@"HMIClusteringTaskSummary",R,V_summary
startTime
T@"NSDate",R,V_startTime
personsModelManager
T@"HMIPersonsModelManager",R,V_personsModelManager
Error fetching persons, error:%@
Fetched %lu persons
Error fetching facecrops for person:%@, error:%@
Fetched %lu face crops for person: %@
Ignoring error fetching faceprints for person:%@, error:%@
Error faceprinting face crops for person:%@, error:%@
Removing existing faceprints at other versions: %@
Failed to generate persons model, error:%@
Invalid configuration: isExternalLibrary is NO but self.dataSource does not conform to HMIHomePersonManagerDataSource protocol!
Successfully updated persons model
Error fetching faces to subsample for %@: %@
Fetched 0 training faces for %@, this would remove all face crops! Skipping face crop removal.
Subsampling will retain %lu from a total of %lu faces for %@
@"NSUUID"16@?0@"VNFaceObservation"8
@"NSUUID"16@?0@"NSUUID"8
Deleting a total of %lu face crops after subsampling
Selected %lu persons for subsampling faces but did not choose any face crops to delete!
update.persons.model.task
removeExcessFaceCrops
TB,R,GshouldRemoveExcessFaceCrops,V_removeExcessFaceCrops
Error fetching faceprints for face crop UUIDs:%@, error:%@
fetch.faceprints.operation
faceCropUUIDs
T@"NSSet",R,V_faceCropUUIDs
faceprints
T@"NSSet",R,V_faceprints
Storing faceprints:%@ failed with error:%@
Storing faceprints:%@ completed successfully
store.faceprints.operation
Removing faceprints:%@ failed with error:%@
Removing faceprints:%@ completed successfully
remove.faceprints.operation
T@"NSSet",R,V_faceprintUUIDs
Removing faceCrops:%@ failed with error:%@
Removing faceCrops:%@ completed successfully
remove.facecrops.operation
cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_cropRect
T@"HMICameraVideoFrame",R,W,V_frame
T@"NSError",R,V_error
v32@?0@"NSNumber"8@16^B24
HMICameraVideoFrameAnalyzerSignificantActivity
Face Classification is enabled, but homeUUID is nil, skipping face recognition
AnalyzerEvents: [%lu/%lu] %@
v24@?0@"HMIVideoAnalyzerEvent"8^B16
none
@"NSString"16@?0@"NSObject"8
high
medium
Unexpected class event %@
q24@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16
v32@?0@"HMIVideoAnalyzerEvent"8Q16^B24
Face classification skipped for stationary or low confidence face: %@, removing face event from person event
ClassifyFaceEvent
Face classification failed for face: %@, error: %@, removing face event from person event
Face: %@ didn't produce any classifications
camera.video.frame.analyzer.significant.activity
classHierarchyMap
mediumConfidenceThresholds
T@"NSDictionary",R,V_mediumConfidenceThresholds
highConfidenceThresholds
T@"NSDictionary",R,V_highConfidenceThresholds
regionOfInterestOperationQueue
T@"NSOperationQueue",R,V_regionOfInterestOperationQueue
regionOfInterestOperations
T@"NSMapTable",R,V_regionOfInterestOperations
significantActivityDetector
T@"HMISignificantActivityDetector",R,V_significantActivityDetector
faceClassifier
T@"HMIFaceClassifierVIP",R,V_faceClassifier
transaction
T@"HMFOSTransaction",&,N,V_transaction
sessionEntityManagers
T@"NSCache",R,V_sessionEntityManagers
HMIDESBT.u
HMIDESBackgroundTask
Unable to archive task %@: %@
T@"NSURL",R,V_url
HMIVideoFrame
scale > 0 && scale <= 1
quality > 0 && quality <= 1
HMIVideoFrame failed to create compressed representation.
]1337;File=inline=1;preserveAspectRatio=1:%@
Data (JPEG)
CVPixelBuffer
Presentation Time Stamp
Size
Store
HMIVideoFrame does not support NSSecureCoding in the simulator.
presentationTimeStamp
T{?=qiIq},R,V_presentationTimeStamp
[Model loading] Error opening encrypted file
[Model loading] Error reading mmaped encrypted file
[Model loading] Error reading data from compressed file
[Model loading] returning unsuccessfully post decompression due to invalid destination size
[Model loading] Error in out file.
/scratch.data
failed stat %s
[Model loading] failed to unpackage resources
[Model loading] unpackaged resources version
HMICompressionHelper
/Library/Spotlight/Backup/temp_%lu.dat
Error in opening temporary file.
preallocated temporary file failed. %d
Serious error in writing temporary file. %d
HMIVideoBuffer
T@"<HMIVideoCommandBufferDelegate>",W,V_delegate
videoSampleCount
videoDelay
videoDuration
bufferWillHandleSampleBuffer
T@?,C,V_bufferWillHandleSampleBuffer
bufferWillFlush
T@?,C,V_bufferWillFlush
bufferWillFinish
T@?,C,V_bufferWillFinish
HMIVideoAnalyzerEventPet
P(%@|[%@])=%@
HMIVideoAnalyzerEvent
Events file "%@" does not exist.
Cannot read events from file "%@", error: %@
Person
Vehicle
Motion
Cannot load events file, exception: %@
Event
eventClasses
T@"HMIConfidence",R,V_confidence
userInfo
T@"NSDictionary",R,V_userInfo
hasMotionVectors
TB,R,V_hasMotionVectors
confidenceLevel
HMIVideoDecoder
Cannot reorder frames.
Decoded:  %@
Video decoder is not running, ignoring %@
Sample buffer has no samples, skipping.
Invalid DTS, expected > %@, got %@, skipping.
Format description is missing.
Cannot create decoder.
Cannot decode frame, err: %d.
T@"<HMIVideoDecoderDelegate>",W,V_delegate
decoderDidDecodeSampleBuffer
T@?,C,V_decoderDidDecodeSampleBuffer
decoderDidFailWithError
T@?,C,V_decoderDidFailWithError
Frame decode error %d
HMIVideoAssetWriter
HMIVideoAssetWriterOutput
This operation cannot be completed because the asset writer has already started.
UTType class is not available.
Cannot add video input.
Cannot add audio input.
Failed to start writing, assetWriter.status: %ld, assetWriter:.error: %@
Started writing at %@
didOutputSegmentData segmentType: %ld
B16@?0@"AVAssetSegmentTrackReport"8
HMICMSampleBufferIsAudio(sbuf)
Trying to recover by adjusting trim duration from %@ to: %@
Asset writer has failed fatally, ignoring %@
Failed to start asset writer.
Underlying asset writer has failed.
Failed to restart asset writer.
Restarted asset writer.
Dropped  %@ because an input for the media type was not found.
Dropped  %@ because asset writer is waiting for a sync sample.
Dropped  %@ because of input error %@
Dropped  %@ because an input %@ is not ready for more media data.
Asset writer has failed fatally, ignoring flush.
Finished waiting for flushSegment, assetWriter.status: %ld, assetWriter.error: %@
Failed to flush segment.
T@"<HMIVideoAssetWriterDelegate>",W,V_delegate
allowRecovery
TB,V_allowRecovery
nextSequenceNumber
TQ,V_nextSequenceNumber
allowRecoveryFromInsufficientAudioTrim
TB,V_allowRecoveryFromInsufficientAudioTrim
assetWriterDidOutputInitializationSegment
T@?,C,V_assetWriterDidOutputInitializationSegment
assetWriterDidOutputSeparableSegment
T@?,C,V_assetWriterDidOutputSeparableSegment
assetWriterDidFailWithError
T@?,C,V_assetWriterDidFailWithError
B16@?0@"AVAssetWriterInput"8
Number of all face observations: %ld
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8Q16^B24
@"NSDictionary"24@?0@"NSUUID"8@"HMIPersonsModel"16
Invalid entry in userDefinedPersonLinks: %@
All links for %@ in userDefinedPersonLinks are invalid
v16@?0@"HMIPersonSourceUUIDPair"8
Skipping person who belongs to user defined equivalency cell: %@
Comparing persons (%@, %@)
Equivalency determined between pair: (%@, %@)!
Cannot add to matching equivalency cell because it already has entry from this source: %@
v32@?0@"NSUUID"8@"NSSet"16^B24
v32@?0@"NSUUID"8@"NSDictionary"16^B24
v24@?0@"VNFaceObservation"8^B16
HMIPersonsModelEquivalencyTable
personToEquivalencyCell
T@"NSDictionary",R,V_personToEquivalencyCell
Failed to update persons model, error:%@, retrying...
Failed to update persons model, error:%@
Submitted persons model update task, taskID:%u, retryOnError:%@
update.persons.model.operation
origin
T{CGPoint=dd},R,V_origin
target
T{CGPoint=dd},R
midpoint
motion
T{CGVector=dd},R,V_motion
eventType
Tq,V_eventType
Time %@
Sparse Optical Flow
camera.video.sparse.optical.flow
T@"NSMutableArray",R,V_frames
sbuf
T^{opaqueCMSampleBuffer=},R,V_sbuf
HMIVideoFrameSelector
Reference: %@, Target: %@
Adding Candidate: %@
q24@?0@"HMIVideoFrameSelectorFrameCandidate"8@"HMIVideoFrameSelectorFrameCandidate"16
Candidates: %@
v16@?0@"HMIVideoFrameSelectorFrameCandidate"8
Selected: %@
Setting sampleRate: %f, referenceInterval: %@, targetInterval: %@
T@"<HMIVideoFrameSelectorDelegate>",W,V_delegate
frameSelectorDidSelectFrame
T@?,C,V_frameSelectorDidSelectFrame
HMIFR.c
HMIFR.fc
HMIFR.fp
HMIFR.fqs
HMIFR.sea
TransitionMatrix
Joint
Face Classifications
Face Quality Score
HMIFaceRecognition
predictedLinkedEntityUUIDs
T@"NSSet",R,V_predictedLinkedEntityUUIDs
Tq,R,V_sessionEntityAssignment
classifications
T@"NSSet",R,V_classifications
faceQualityScore
Td,R,V_faceQualityScore
selector
arguments
Creating analyzer with identifier: %@, configuration: %@
HMIVideoAnalyzer does not support remote analysis.
T@"HMIVideoAnalyzerConfiguration",R,C,V_configuration
T@"NSDictionary",R,C,V_options
stateDescription
delay
Td,N
monitored
TB,N
T@"<HMIVideoAnalyzerDelegate>",W,V_delegate
TQ,R,V_status
HMIVideoAnalyzerServer
Recieved Message: %@
Unknown %@
Sent Message Reply: %@
dynamicConfiguration
Video fragment duration: %fs is greater than the max fragment duration value: %fs
Handling: %@
Cancel is not yet implemented.
>> -[HMIVideoAnalyzerServer flush]
v16@?0@"HMIVideoAnalyzerResultFilter"8
Bundling Fragment Result, timeRange: %@, frames: [%@], thumbnails [%@]
Frame analyzer result buffer should be empty. %@
Thumbnail buffer should be empty. %@
@16@?0@"HMIVideoFrameAnalyzerResult"8
Analyzer Failed: %@
Sending Result: %@
 %.2f
: buffer: %4.1fs %3ldf
, current: %4.1fs %3ldf
, results: [%ld, %ld]
, frame: %@
, delay: %@
, enc: %@
, res: %lu
, thb: %lu
analyzerDidAnalyzeFrame
T@?,C,V_analyzerDidAnalyzeFrame
analyzerDidAnalyzeFragment
T@?,C,V_analyzerDidAnalyzeFragment
analyzerDidFailWithError
T@?,C,V_analyzerDidFailWithError
/tmp/com.apple.HomeAI/familiar-face-data
Error initializing with home UUID: %@ source UUID:%@
Error enumerating persons
Error loading person from JSON
Error enumerating face crops for person UUID:%@
Error loading face crop from JSON
Error loading face crop image
person.datasource.disk
sourceURL
T@"NSURL",R,V_sourceURL
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/datastructs.cpp
cvReleaseMemStorage
cvRestoreMemStoragePos
NULL storage pointer
cvMemStorageAlloc
Too large memory block is requested
storage->free_space % CV_STRUCT_ALIGN == 0
requested size is negative or too big
(size_t)ptr % CV_STRUCT_ALIGN == 0
cvCreateSeq
Specified element size doesn't match to the size of the specified element type (try to use 0 for element type)
cvSetSeqBlockSize
Storage block size is too small to fit the sequence elements
cvCvtSeqToArray
cvStartReadSeq
cvChangeSeqBlock
cvGetSeqReaderPos
cvSetSeqReaderPos
cvSeqPush
ptr + elem_size <= seq->block_max
block->start_index > 0
NULL sequence pointer
cvSeqPushMulti
number of removed elements is negative
cvSeqPopMulti
delta > 0
cvClearSeq
Invalid sequence header
cvSeqSlice
Bad sequence slice
Bad input sequence
cvSeqSort
Null compare function
cvCreateSet
cvSetAdd
count <= CV_SET_ELEM_IDX_MASK+1
cvCreateGraph
cvGraphAddVtx
cvFindGraphEdgeByPtr
ofs == 1 || start_vtx == edge->vtx[0]
graph pointer is NULL
cvGraphAddEdgeByPtr
vertex pointers coinside (or set to NULL)
edge->flags >= 0
Invalid graph pointer
cvCloneGraph
cvInitTreeNodeIterator
cvNextTreeNode
icvInitMemStorage
icvDestroyMemStorage
icvGoNextMemBlock
parent->bottom == block
icvGrowSeq
The sequence has NULL storage pointer
storage->free_space >= delta
block->count % seq->elem_size == 0 && block->count > 0
seq->first->start_index == 0
icvFreeSeqBlock
(in_front_of ? block : block->prev)->count == 0
seq->ptr == block->data
block->count > 0 && block->count % seq->elem_size == 0
Unknown/unsupported border type
borderInterpolate
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/filter.cpp
columnBorderType != BORDER_WRAP
!rowFilter.empty() && !columnFilter.empty()
bufType == srcType
0 <= anchor.x && anchor.x < ksize.width && 0 <= anchor.y && anchor.y < ksize.height
roi.x >= 0 && roi.y >= 0 && roi.width >= 0 && roi.height >= 0 && roi.x + roi.width <= wholeSize.width && roi.y + roi.height <= wholeSize.height
start
srcRoi.x >= 0 && srcRoi.y >= 0 && srcRoi.width >= 0 && srcRoi.height >= 0 && srcRoi.x + srcRoi.width <= src.cols && srcRoi.y + srcRoi.height <= src.rows
wholeSize.width > 0 && wholeSize.height > 0
proceed
src && dst && count > 0
srcY >= startY
dstY <= roi.height
src.type() == srcType && dst.type() == dstType
apply
dstOfs.x >= 0 && dstOfs.y >= 0 && dstOfs.x + srcRoi.width <= dst.cols && dstOfs.y + srcRoi.height <= dst.rows
_kernel.channels() == 1
getKernelType
cn == CV_MAT_CN(bufType) && ddepth >= std::max(sdepth, CV_32S) && kernel.type() == ddepth
getLinearRowFilter
Unsupported combination of source format (=%d), and buffer format (=%d)
cn == CV_MAT_CN(bufType) && sdepth >= std::max(ddepth, CV_32S) && kernel.type() == sdepth
getLinearColumnFilter
Unsupported combination of buffer format (=%d), and destination format (=%d)
cn == CV_MAT_CN(_dstType)
createSeparableLinearFilter
ktype == CV_8U || ktype == CV_32S || ktype == CV_32F || ktype == CV_64F
preprocess2DKernel
cn == CV_MAT_CN(dstType) && ddepth >= sdepth
getLinearFilter
Unsupported combination of source format (=%d), and destination format (=%d)
createLinearFilter
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0
SymmColumnVec_32s8u
SymmColumnSmallVec_32s16s
SymmColumnSmallVec_32f
SymmColumnVec_32f16s
SymmColumnVec_32f
anchor.inside(Rect(0, 0, ksize.width, ksize.height))
normalizeAnchor
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/precomp.hpp
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0 && this->ksize <= 5
SymmRowSmallFilter
kernel.type() == DataType<DT>::type && (kernel.rows == 1 || kernel.cols == 1)
RowFilter
kernel.type() == DataType<ST>::type && (kernel.rows == 1 || kernel.cols == 1)
ColumnFilter
this->ksize == 3
SymmColumnSmallFilter
SymmColumnFilter
_kernel.type() == DataType<KT>::type
Filter2D
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/alloc.cpp
Failed to allocate %lu bytes
OutOfMemoryError
borderType != BORDER_CONSTANT
pyrDown
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/pyramids.cpp
!_src.empty()
pyrDown_
std::abs(dsize.width*2 - ssize.width) <= 2 && std::abs(dsize.height*2 - ssize.height) <= 2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/opengl_interop_deprecated.cpp
GlBuffer
GlTexture
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/persistence.cpp
Invalid pointer to file storage
The node is neither a map nor an empty collection
cvGetFileNodeByName
Null element name
cvStartWriteStruct
The file storage is opened for reading
cvEndWriteStruct
cvWriteInt
cvWriteString
cvWriteRawData
Negative number of elements
Null data pointer
cvStartReadRawData
Null pointer to source file node or reader
The file node should be a numerical scalar or a sequence
cvReadRawDataSlice
Null pointer to reader or destination array
The readed sequence is a scalar, thus len must be 1
The sequence element is not a numerical scalar
The sequence slice does not fit an integer number of records
Null pointers to source file node or destination array
cvReadRawData
opencv-sequence
opencv-sequence-tree
opencv-graph
opencv-sparse-matrix
opencv-image
opencv-matrix
opencv-nd-matrix
Invalid type info
cvRegisterType
Some of required function pointers (is_instance, release, read or write) are NULL
Type name should start with a letter or _
Type name should contain only letters, digits, - and _
NULL double pointer
cvRead
The node does not represent a user object (unknown type?)
Unknown array type
The storage is not opened
icvPuts
An attempt to add element without a key to a map, or add element with key to sequence
icvXMLWriteTag
A single _ is a reserved tag name
Closing tag should not include any attributes
Key should start with a letter or _
Key name may only contain alphanumeric characters [a-zA-Z0-9], '-' and '_'
icvDecodeFormat
fmt_pairs != 0 && max_len > 0
Invalid data type specification
Too long data type specification
(align & (align-1)) == 0 && size < INT_MAX
%.8e
-.Inf
.Inf
%.16e
elements with keys can not be written to sequence
icvXMLWriteScalar
icvYMLWrite
The key is an empty
The key is too long
Key must start with a letter or _
Key names may only contain alphanumeric characters [a-zA-Z0-9], '-', '_' and ' '
icvReleaseSeq
flags
count
Some of essential sequence attributes are absent
icvReadSeq
The sequence flags are invalid
curve
closed
hole
untyped
header_dt
header_user_data
One of "header_dt" and "header_user_data" is there, while the other is not
rect
Only one of "header_user_data", "rect" and "origin" tags may occur
color
data
The image data is not found in file storage
The number of stored elements does not match to "count"
Too complex format for the matrix
icvDecodeSimpleFormat
recursive
false
False
FALSE
icvWriteSeqTree
CV_IS_SEQ( seq )
sequences
icvWriteSeq
level
The size of element calculated from "dt" and the elem_size do not match
icvGetFormat
Size of sequence element (elem_size) is inconsistent with seq->flags
%d%c
The size of header calculated from "header_dt" is greater than header_size
icvWriteHeaderData
opencv-sequence-tree instance should contain a field "sequences" that should be a sequence
icvReadSeqTree
All the sequence tree nodes should contain "level" field
level == prev_level + 1
icvReleaseGraph
vertex_dt
edge_dt
vertex_count
edge_count
Some of essential graph attributes are absent
icvReadGraph
oriented
Graph edges should start with 2 integers and a float
%df%s
vertices
edges
No edges data
No vertices data
Some of stored vertex indices are out of range
Duplicated edge has occured
cvAlignPtr
(align & (align-1)) == 0
icvWriteGraph
CV_IS_GRAPH(graph)
2if%s
sizes
Some of essential matrix attributes are absent
icvReadSparseMat
Could not determine sparse matrix dimensionality
The matrix data is not found in file storage
Sparse matrix data is corrupted
icvWriteSparseMat
CV_IS_SPARSE_MAT(mat)
(reader).seq->elem_size == sizeof(idx)
k < dims
Some of essential image attributes are absent
icvReadImage
layout
interleaved
Only interleaved images can be read
The matrix size does not match to the number of stored elements
icvWriteImage
CV_IS_IMAGE(image)
Images with planar data layout are not supported
top-left
bottom-left
planar
rows
cols
icvReadMat
icvWriteMat
CV_IS_MAT_HDR_Z(mat)
icvReadMatND
Could not determine the matrix dimensionality
icvWriteMatND
CV_IS_MATND_HDR(mat)
type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/dxt.cpp
This mode (using nonzero_rows with a single-column matrix) breaks the function's logic, so it is prohibited.
For fast convolution/correlation use 2-column matrix or single-row matrix instead
!inv
type == srcB.type() && srcA.size() == srcB.size()
mulSpectrums
(flags & DFT_NO_PERMUTE) == 0
(unsigned)k0 < (unsigned)n && (unsigned)k1 < (unsigned)n
factors[0] == factors[nf-1]
(unsigned)j < (unsigned)n2
(unsigned)j < (unsigned)n
RealDFT
tab_size == n
CCSIDFT
src != dst
DFTInit
nf < 34
elem_size == sizeof(Complex<float>)
channels() == CV_MAT_CN(dtype)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/copy.cpp
mask.depth() == CV_8U && (mcn == 1 || mcn == cn)
size() == mask.size()
checkScalar(value, type(), _value.kind(), _InputArray::MAT )
setTo
mask.empty() || mask.type() == CV_8U
repeat
ny > 0 && nx > 0
maskarr == 0
cvCopy
src.depth() == dst.depth() && src.size == dst.size
(coi1 != 0 || src.channels() == 1) && (coi2 != 0 || dst.channels() == 1)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/opengl_interop.cpp
The library is compiled without OpenGL support
throw_nogl
threshold
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/thresh.cpp
Unknown threshold type
thresh_8u
thresh_16s
thresh_32f
img.depth() == CV_8U && winSize.width > 2 && winSize.height > 2
buildOpticalFlowPyramid
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/video/lkpyramid.cpp
maxLevel >= 0 && winSize.width > 2 && winSize.height > 2
calcOpticalFlowPyrLK
(npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0
nextPtsMat.checkVector(2, CV_32F, true) == npoints
statusMat.isContinuous()
errMat.isContinuous()
levels1 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + prevPyr[lvlStep1].cols + winSize.width <= fullSize.width && ofs.y + prevPyr[lvlStep1].rows + winSize.height <= fullSize.height
levels2 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + nextPyr[lvlStep2].cols + winSize.width <= fullSize.width && ofs.y + nextPyr[lvlStep2].rows + winSize.height <= fullSize.height
prevPyr[level * lvlStep1].size() == nextPyr[level * lvlStep2].size()
prevPyr[level * lvlStep1].type() == nextPyr[level * lvlStep2].type()
depth == CV_8U
calcSharrDeriv
cvAlign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/include/opencv2/core/internal.hpp
scn == 1
convertAndUnrollScalar
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/arithm.cpp
op == CMP_LT || op == CMP_LE || op == CMP_EQ || op == CMP_NE || op == CMP_GE || op == CMP_GT
compare
The operation is neither 'array op array' (where arrays have the same size and the same type), nor 'array op scalar', nor 'scalar op array'
The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array'
binary_op
(mask.type() == CV_8UC1 || mask.type() == CV_8SC1)
mask.size == src1.size
operator()
-256 <= ((b) - (a)) && ((b) - (a)) <= 512
-256 <= ((a) - (b)) && ((a) - (b)) <= 512
The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array'
arithm_op
src2.type() == CV_64F && (src2.rows == 4 || src2.rows == 1)
When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified
-256 <= (a - b) && (a - b) <= 512
img.dims <= 2 && templ.dims <= 2 && corr.dims <= 2
crossCorr
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/templmatch.cpp
depth == tdepth || tdepth == CV_32F
corrsize.height <= img.rows + templ.rows - 1 && corrsize.width <= img.cols + templ.cols - 1
ccn == 1 || delta == 0
the input arrays are too big
CV_MAT_CN(sumType) == CV_MAT_CN(srcType)
getRowSumFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/smooth.cpp
CV_MAT_CN(sumType) == CV_MAT_CN(dstType)
getColumnSumFilter
Unsupported combination of sum format (=%d), and destination format (=%d)
sumCount == ksize-1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/utils.cpp
top >= 0 && bottom >= 0 && left >= 0 && right >= 0
copyMakeBorder
value[0] == value[1] && value[0] == value[2] && value[0] == value[3]
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/deriv.cpp
ktype == CV_32F || ktype == CV_64F
getScharrKernels
dx >= 0 && dy >= 0 && dx+dy == 1
getSobelKernels
The kernel size must be odd and not larger than 31
dx >= 0 && dy >= 0 && dx+dy > 0
ksize > order
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/convert.cpp
src && nsrcs > 0 && dst && ndsts > 0 && fromTo && npairs > 0
mixChannels
j < nsrcs && src[j].depth() == depth
i1 >= 0 && j < ndsts && dst[j].depth() == depth
type == B.type() && (type == CV_32FC1 || type == CV_64FC1 || type == CV_32FC2 || type == CV_64FC2)
gemm
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/matmul.cpp
a_size.width == len
a_size.height == len
C.type() == type && (((flags&GEMM_3_T) == 0 && C.rows == d_size.height && C.cols == d_size.width) || ((flags&GEMM_3_T) != 0 && C.rows == d_size.width && C.cols == d_size.height))
type == CV_64FC2
src1.type() == src2.type()
scaleAdd
src.channels() == 1
mulTransposed
delta.channels() == 1 && (delta.rows == src.rows || delta.rows == 1) && (delta.cols == src.cols || delta.cols == 1)
GEMM_TransposeBlock
MulTransposedR
delta_cols == 1
cn <= 4 && func != 0
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/stat.cpp
src.channels() == 1 && func != 0
countNonZero
mean
(cn == 1 && (mask.empty() || mask.type() == CV_8U)) || (cn >= 1 && mask.empty() && !minIdx && !maxIdx)
minMaxIdx
img.dims <= 2
src.type() == CV_8UC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/lapack.cpp
type == CV_32F || type == CV_64F
invert
m == n
method == DECOMP_LU || method == DECOMP_CHOLESKY
n == 1
type == _src2.type() && (type == CV_32F || type == CV_64F)
solve
(method != DECOMP_LU && method != DECOMP_CHOLESKY) || is_normal || src.rows == src.cols
src.rows == 1
The function can not solve under-determined linear systems
src.rows == src.cols
eigen
w.type() == u.type() && u.type() == vt.type() && u.data && vt.data && w.data
backSubst
u.cols >= nm && vt.rows >= nm && (w.size() == Size(nm, 1) || w.size() == Size(1, nm) || w.size() == Size(vt.rows, u.cols))
rhs.data == 0 || (rhs.type() == type && rhs.rows == m)
_SVDcompute
0 <= d && d <= CV_MAX_DIM && _sizes
create
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/matrix.cpp
step[dims-1] == (size_t)CV_ELEM_SIZE(flags)
m.dims >= 2
0 <= _rowRange.start && _rowRange.start <= _rowRange.end && _rowRange.end <= m.rows
0 <= _colRange.start && _colRange.start <= _colRange.end && _colRange.end <= m.cols
m.dims <= 2
0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows
ranges
r == Range::all() || (0 <= r.start && r.start < r.end && r.end <= m.size[i])
dims <= 2
diag
img->dataOrder == IPL_DATA_ORDER_PIXEL
img->dataOrder == IPL_DATA_ORDER_PIXEL || img->roi->coi != 0
(int)nelems >= 0
reserve
resize
COI is not supported by the function
cvarrToMat
seq->total > 0 && CV_ELEM_SIZE(seq->flags) == seq->elem_size
dims <= 2 && step[0] > 0
locateROI
adjustROI
reshape
The matrix is not continuous, thus its number of rows can not be changed
Bad new number of rows
The total number of matrix elements is not divisible by the new number of rows
The total width is not divisible by the new number of channels
cn <= 4
scalarToRawData
i < 0
getMat
0 <= i && i < (int)vv.size()
This method is not implemented for oclMat yet
k == STD_VECTOR_MAT
0 <= i && i < (int)v.size()
getMatVector
This function in deprecated, do not use it
getGlBuffer
getGlTexture
k == GPU_MAT
getGpuMat
i < (int)vv.size()
total
empty
!fixedSize() || ((Mat*)obj)->size.operator()() == _sz
!fixedType() || ((Mat*)obj)->type() == mtype
!fixedSize() || ((gpu::GpuMat*)obj)->size() == _sz
!fixedType() || ((gpu::GpuMat*)obj)->type() == mtype
!fixedSize() || ((ogl::Buffer*)obj)->size() == _sz
!fixedType() || ((ogl::Buffer*)obj)->type() == mtype
!fixedSize() || ((Mat*)obj)->size.operator()() == Size(cols, rows)
!fixedSize() || ((gpu::GpuMat*)obj)->size() == Size(cols, rows)
!fixedSize() || ((ogl::Buffer*)obj)->size() == Size(cols, rows)
!fixedType() && !fixedSize()
CV_MAT_TYPE(mtype) == m.type()
m.dims == dims
m.size[j] == sizes[j]
mtype == type0 || (CV_MAT_CN(mtype) == 1 && ((1 << type0) & fixedDepthMask) != 0)
dims == 2 && ((sizes[0] == sz.height && sizes[1] == sz.width) || (allowTransposed && sizes[0] == sz.width && sizes[1] == sz.height))
dims == 2 && (sizes[0] == 1 || sizes[1] == 1 || sizes[0]*sizes[1] == 0)
!fixedSize() || len == vv.size()
mtype == type0 || (CV_MAT_CN(mtype) == CV_MAT_CN(type0) && ((1 << type0) & fixedDepthMask) != 0)
!fixedSize() || len == ((vector<uchar>*)v)->size() / esz
Vectors with element size %d are not supported. Please, modify OutputArray::create()
create() called for the missing output array
!fixedSize() || len == len0
v[j].empty()
i < (int)v.size()
!fixedType() || (CV_MAT_CN(mtype) == m.channels() && ((1 << CV_MAT_TYPE(flags)) & fixedDepthMask) != 0)
!fixedSize()
release
clear
k == MAT
getMatRef
setIdentity
src.dims <= 2 && esz <= (size_t)32
transpose
src.size() == dst.size() && (src.cols == 1 || src.rows == 1)
func != 0
m.dims <= 2 && m.rows == m.cols
completeSymm
src.dims <= 2
src.channels() == dst.channels()
_arrays && (_ptrs || _planes)
init
narrays <= 1000
arrays[i] != 0
A.size == arrays[i0]->size
A.step[d-1] == A.elemSize()
copyTo
convertTo
minMaxLoc
0 <= _dims && _dims <= CV_MAX_DIM
setSize
s >= 0
%s:%d: error: (%d) %s in function %s
%s:%d: error: (%d) %s
OpenCV Error: %s (%s) in %s, file %s, line %d
No Error
Backtrace
Unspecified error
Internal error
Insufficient memory
Bad argument
Iterations do not converge
Autotrace call
Incorrect size of input array
Null pointer
Division by zero occured
Image step is wrong
Inplace operation is not supported
Requested object was not found
Input image depth is not supported by function
Formats of input arguments do not match
Sizes of input arguments do not match
One of arguments' values is out of range
Unsupported format or combination of formats
Input COI is not supported
Bad number of channels
Bad flag (parameter or structure field)
Bad parameter of type CvPoint
Bad type of mask argument
Parsing error
The function/feature is not implemented
Memory block has been corrupted
Assertion failed
No GPU support
Gpu API call
No OpenGL support
OpenGL API call
Unknown %s code %d
status
module != 0 && module->name != 0 && module->version != 0
cvRegisterModule
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/system.cpp
cxcore
2.4.13.6
unknown function
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/include/opencv2/dynamicuda/dynamicuda.hpp
copy
copyWithMask
convert
mallocPitch
The library is compiled without CUDA support
qualityLevel > 0 && minDistance >= 0 && maxCorners >= 0
goodFeaturesToTrack
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/featureselect.cpp
mask.empty() || (mask.type() == CV_8UC1 && mask.size() == image.size())
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/array.cpp
Non-positive width or height
cvCreateMatHeader
Invalid matrix type
cvInitMatHeader
Non-positive cols or rows
cvReleaseMat
Bad CvMat header
cvCloneMat
NULL matrix header pointer
cvInitMatNDHeader
invalid array data type
NULL <sizes> pointer
non-positive or too large number of dimensions
one of dimesion sizes is non-positive
The array is too big
cvCreateMatNDHeader
Bad CvMatND header
cvCloneMatND
src->dims <= CV_MAX_DIM
_dst.data == data0
Incorrect number of arrays
cvInitNArrayIterator
Some of required array pointers is NULL
Iterator pointer is NULL
COI set is not allowed here
Number of dimensions is the same for all arrays
Data type is not the same for all arrays
Number of channels is not the same for all arrays
Depth is not the same for all arrays
Mask should have 8uC1 or 8sC1 data type
Dimension sizes are the same for all arrays
cvNextNArraySlice
iterator != 0
cvCreateSparseMat
bad number of dimensions
cvReleaseSparseMat
Invalid sparse array header
cvCloneSparseMat
Invalid sparse matrix header
cvInitSparseMatIterator
NULL iterator pointer
Data is already allocated
cvCreateData
unrecognized or unsupported array type
cvReleaseData
Only continuous nD arrays are supported here
cvGetElemType
cvGetDims
Array should be CvMat or IplImage
cvGetSize
index is out of range
cvPtr2D
COI must be non-null in case of planar images
NULL pointer to indices
cvPtrND
NULL array pointer is passed
cvGetMat
The matrix has NULL data pointer
The image has NULL data pointer
Images with planar data layout should be used with COI selected
The image is interleaved and has over CV_CN_MAX channels
Pixel order should be used with coi == 0
Input array has NULL data pointer
Unrecognized or unsupported array type
cvCreateImage
null pointer to header
cvInitImageHeader
Bad input roi
Unsupported format
Bad input origin
Bad input align
cvReleaseImageHeader
cvReleaseImage
cvSetImageROI
rect.width >= 0 && rect.height >= 0 && rect.x < image->width && rect.y < image->height && rect.x + rect.width >= (int)(rect.width > 0) && rect.y + rect.height >= (int)(rect.height > 0)
cvSetImageCOI
cvGetImageCOI
Bad image header
cvCloneImage
cvGetMatND
icvGetNodePtr
CV_IS_SPARSE_MAT( mat )
One of indices is out of range
(newsize & (newsize - 1)) == 0
GRAY
BGRA
op == MORPH_ERODE || op == MORPH_DILATE
getMorphologyRowFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/morph.cpp
Unsupported data type (=%d)
getMorphologyColumnFilter
getMorphologyFilter
depth == CV_8U || depth == CV_16U || depth == CV_16S || depth == CV_32F || depth == CV_64F
createMorphologyFilter
shape == MORPH_RECT || shape == MORPH_CROSS || shape == MORPH_ELLIPSE
getStructuringElement
morphOp
((size_t)src[i] & 15) == 0
((size_t)_src[i] & 15) == 0
_kernel.type() == CV_8U
MorphFilter
src.type() == CV_8UC1 || src.type() == CV_32FC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/imgproc/corner.cpp
cornerEigenValsVecs
CV_MAT_CN(_type) == e.a.channels()
assign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-141.2.4/OpenCV/src/core/matop.cpp
Unknown operation
Invalid matrix initializer type
%{public}@%{public}@
%{public}@%{public}s
Identifier = %@, Name = %@
HMISignpost
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
HMIExternalPersonDataSourceDisk
HMIExternalPersonManagerDataSource
HMIPersonManagerDataSource
NSObject
HMIMutableCluster
HMFLogging
HMIInputFeatureProvider
MLFeatureProvider
HMISignificantActivityDetector
HMISystemResourceUsage
HMISystemResourceUsageMonitor
HMISystemResourceUsageMonitorProtocol
HMIVideoAnalyzerEventFace
HMIDataReader
HMIVideoEventEntry
HMIVideoEvent
HMIVideoEventBuffer
HMIVideoTimelineEntry
HMIVideoTimeline
HMIVideoTimelineProfiler
HMITimeIntervalAverage
HMIVideoAnalyzerEventMotion
HMIHomePersonManager
HMFTimerDelegate
HMIStoreFaceCropOperation
HMIStoreFaceprintOperation
HMICameraVideoFrame
HMIVisionUtilities
HMIThermalMonitorService
HMIThermalMonitor
HMIMemorySampler
HMIPBSSystemLoadMonitor
HMIVideoAssetReader
HMIHomeKitClient
HMHomeManagerDelegateAdapter
HMHomeManagerDelegate
HMISignpost
HMIFaceQualityFilterModelInput
HMIFaceQualityFilterSVM
HMIVideoNode
HMIVideoProcessingNode
PFLBackgroundRunner
_DASExtensionRunner
HMIExternalPersonDataSourceHomeKit
HMI_CVML_Error
HMIPersonFaceCrop
NSCopying
NSSecureCoding
NSCoding
HMIVideoAnalyzerFrameResult
HMIMemoryAVAsset
AVAssetResourceLoaderDelegate
MovingAverageEntry
MovingAverage
HMICameraVideoAnalyzerResult
HMICameraVideoAnalyzerConfiguration
HMICameraVideoAnalyzerSignificantEvent
HMICameraVideoFrameResult
HMICameraVideoFragment
HMICameraVideoAnalyzerHistory
HMICameraVideoAnalyzerRequest
HMIVideoEncoderDelegate
HMIVideoRetimerDelegate
HMIVideoFrameSamplerDelegate
HMICameraVideoFrameSelectorDelegate
HMICameraVideoAnalyzerScheduler
HMISystemResourceUsageMonitorDelegate
HMICameraVideoAnalyzer
HMIDESMutableFloatArray
HMIFaceClassifierVIP
HMIFaceClassifier
HMICameraVideoFrameAnalyzerFactory
HMICIFilterAttributeValue
HMICIFilterAttribute
HMICIFilter
HMIDESDatasetSample
HMIDESDataset
HMINMSConfiguration
HMIObjectDetection
HMIObjectDetectionUtils
HMIVideoFrameAnalyzerResult
HMIVideoFrameAnalyzer
HMIVideoFrameAnalyzerDelegateAdapter
HMIVideoFrameAnalyzerDelegate
HMICameraVideoPosterFrame
HMICameraVideoPosterFrameGeneratorInput
HMICameraVideoPosterFrameGenerator
HMIFaceCrop
HMIVideoRetimer
HMIVideoRetimerDelegateAdapter
HMICamera
HMITask
HMIHomeTask
HMIEmptyTask
HMITaskServiceServer
HMITaskService
HMIFetchPersonsOperation
HMFObject
HMICameraVideoFrameSelectorFrameScore
HMICameraVideoFrameSelector
HMICameraVideoFrameSamplerDelegate
HMICameraVideoFrameSampler
HMIUpdatedFaceprintsResult
HMIFaceprinter
HMIFaceDetectorVision
HMIFaceDetector
HMIFaceprint
HMIPoint
HMIHomePersonManagerSettings
NSMutableCopying
HMIMutableHomePersonManagerSettings
HMIModelLoader
HMIFeedbackSession
NSURLSessionDelegate
HMIFeedbackSubmitClipOperation
HMIFeedbackTask
HMIFeedback
HMIFaceClassification
HMIPersonsModelsSummaryTask
HMIFaceMisclassificationTask
HMIRemovePersonsModelTask
HMIPersonManager
HMIConfidence
HMIError
HMIVideoFragment
HMISessionEntityManager
HMITuriTrialUpdateTask
HMIVideoAnnotator
HMISystemResourceUsageMonitorImpl
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPersonSourceUUIDPair
HMIPerson
HMIPersonsModelSummary
HMIPersonsModel
HMIJSONArchiver
HMIJSONUnarchiver
HMIFaceUtilities
HMIPersonBlob
HMIFaceTrackerMatch
HMIFaceTrackerMotionRecord
HMIFaceTracker
HMICameraVideoResourceAttributes
HMICameraVideoAssetReader
HMIVideoEncoder
HMIVideoEncoderDelegateAdapter
HMIHomePersonDataSourceHomeKit
HMIHomePersonManagerDataSource
HMIExternalPersonManagerSettings
HMIMutableExternalPersonManagerSettings
HMIAnalytics
HMIPersonsModelsSummary
HMIPersonsModelPrediction
HMIPersonsModelManager
HMIDESLayerParameters
HMIDESTrainingResult
HMIDESTrainer
HMIVideoAnalyzerResultOutcome
HMIExternalPersonManager
HMIRemovePersonsModelOperation
HMIVideoAnalyzerScheduler
HMITuriTrialManager
HMIGreedyClustering
HMIVideoAnalyzerLegacy
HMICameraVideoAnalyzerDelegate
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
HMIVideoFrameSamplerDelegateAdapter
HMIVideoPackageAnalyzer
HMIVideoPackageAnalyzerDelegateAdapter
HMIVideoPackageAnalyzerDelegate
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
HMIFaceQualityScoreConsolidated
HMIVideoAnalyzerFragmentResult
HMICameraActivityZone
HMIVideoAnalyzerConfiguration
HMIVideoAnalyzerDynamicConfiguration
HMIMotionDetection
HMICameraVideoAnalyzerDelegateAdapter
HMIAnalysisService
HMIVideoAnalyzerEventPerson
HMIDESRunner
HMIPreference
HMIHomePersonDataSourceDisk
HMIFetchPersonFaceCropsOperation
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
HMIClusteringTaskSummary
HMIHomePersonClusteringTask
HMIUpdatePersonsModelTask
HMIFetchFaceprintsForFaceCropsOperation
HMIStoreFaceprintsOperation
HMIRemoveFaceprintsOperation
HMIRemoveFaceCropsOperation
HMIRegionOfInterestOperation
HMICameraVideoFrameAnalyzerSignificantActivity
HMICameraVideoFrameAnalyzer
HMIDESBackgroundTask
HMIVideoFrame
HMICompressionHelper
HMIVideoCommandBuffer
HMIVideoCommandBufferDelegateAdapter
HMIVideoCommandBufferDelegate
HMIVideoAnalyzerEventPet
HMIVideoAnalyzerEvent
HMIVideoDecoder
HMIVideoDecoderDelegateAdapter
HMIVideoDecoderDelegate
HMIVideoAssetWriter
AVAssetWriterDelegate
HMIVideoAssetWriterDelegateAdapter
HMIVideoAssetWriterDelegate
HMIPersonsModelEquivalencyTable
HMIUpdatePersonsModelOperation
HMISparseOpticalFlowFeatureVector
HMISparseOpticalFlowFrame
HMISparseOpticalFlowMotionDetection
HMISparseOpticalFlowMotionDetector
HMIMotionDetector
HMIVideoFrameSelectorFrameCandidate
HMIVideoFrameSelector
HMIVideoFrameSelectorDelegateAdapter
HMIVideoFrameSelectorDelegate
HMIFaceRecognition
HMIVideoAnalyzer
HMIVideoAnalyzerDelegatePrivate
HMIVideoAnalyzerDelegate
HMIVideoAnalyzerServer
HMIVideoAnalyzerDelegateAdapter
HMIPersonDataSourceDisk
workQueue
sourceURL
UUID
UUIDString
URLByAppendingPathComponent:
defaultManager
path
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
stringWithFormat:
hmiPrivateErrorWithCode:description:underlyingError:
name
dictionaryWithObjects:forKeys:count:
serializeJSONObject:url:error:
countByEnumeratingWithState:objects:count:
personUUID
faceBoundingBox
dateCreated
dataRepresentation
writeToURL:atomically:
hmiPrivateErrorWithCode:description:
hmfErrorWithCode:
categoryWithName:
logCategory
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
fetchAllPersonsWithCompletion:
fetchPersonsWithUUIDs:completion:
fetchAllPersonFaceCropsWithCompletion:
fetchFaceCropsForPersonsWithUUIDs:completion:
fetchAllFaceprintsWithCompletion:
fetchFaceprintsForFaceCropsWithUUIDs:completion:
performCloudPullWithCompletion:
addFaceprints:completion:
removeFaceprintsWithUUIDs:completion:
fetchSettingsWithCompletion:
addPerson:completion:
addPersonFaceCrops:completion:
init
data
initWithData:
arrayWithObject:
count
copy
addObjectsFromArray:
scale:
add:
addObject:
floatArrayByScaling:
logIdentifier
initWithFaceprint:
faceprintUUIDs
addLinkedEntityUUIDs:
linkedEntityUUIDs
addFaceprints:
centroid
.cxx_destruct
_faceprintUUIDs
_linkedEntityUUIDs
_centroid
pixelBuffer
dealloc
inputName
arrayWithObjects:count:
setWithArray:
isEqualToString:
featureValueWithPixelBuffer:
featureValueForName:
featureNames
initWithPixelBuffer:inputName:
_pixelBuffer
_inputName
objectAtIndexedSubscript:
doubleValue
sharedInstance
usesCPUOnly
setUsesCPUOnly:
boolPreferenceForKey:defaultValue:
setAllowBackgroundGPUCompute:
fileURLWithPath:
modelWithContentsOfURL:configuration:error:
hmiPrivateErrorWithCode:underlyingError:
arrayWithCapacity:
initWithName:
_runNeuralNetworkOnPixelBuffer:offsets:scores:yaws:rolls:error:
_postProcessOffsets:scores:yaws:rolls:outputPredictions:
initWithPixelBuffer:presentationTimeStamp:
printWithScale:
transferPixelBuffer:pixelFormat:options:error:
inputFeatureValueName
mlModel
predictionOptions
predictionFromFeatures:options:error:
offsetsFeatureValueNames
type
multiArrayValue
scoresFeatureValueNames
yawsFeatureValueNames
rollsFeatureValueNames
array
shape
unsignedLongValue
dataPointer
strides
useSoftmax
numberWithDouble:
initWithLabelIndex:confidence:unclampedBoundingBox:yaw:roll:
nmsConfiguration
nmsMultiClass:output:nmsConfiguration:
boundingBox
labelIndex
confidence
roll
initWithLabelIndex:confidence:boundingBox:yaw:roll:
bundleForClass:
pathForResource:ofType:
defaultAssetPath
initWithConfidenceThresholds:nmsConfiguration:assetPath:error:
predict:detectedObjects:error:
inputDimensions
_confidenceThresholds
_anchorSizes
_numberOfAnchors
_useSoftmax
_mlModel
_inputFeatureValueName
_offsetsFeatureValueNames
_scoresFeatureValueNames
_yawsFeatureValueNames
_rollsFeatureValueNames
_nmsConfiguration
_predictionOptions
_inputDimensions
systemResourceUsageLevel
setSystemResourceUsageLevel:
_systemResourceUsageLevel
UTF8String
productInfo
productClass
initWithProductClass:workQueue:
systemResourceUsageMonitorImpl
setDelegate:
delegate
getCurrentSystemResourceUsage
start
_systemResourceUsageMonitorImpl
_workQueue
initWithConfidence:boundingBox:yaw:roll:faceRecognition:userInfo:
initWithConfidence:boundingBox:hasMotionVectors:userInfo:
value
initWithName:value:
faceRecognition
shortDescription
encodeWithCoder:
encodeObject:forKey:
initWithCoder:
decodeObjectOfClass:forKey:
userInfo
supportsSecureCoding
initWithConfidence:boundingBox:
initWithConfidence:boundingBox:faceRecognition:
attributeDescriptions
_faceRecognition
_yaw
_roll
whitespaceCharacterSet
stringByTrimmingCharactersInSet:
length
bytes
subdataWithRange:
position
readUInt32
readUInt64
seek:
readData:
_data
_position
internal
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
substringToIndex:
stringWithCapacity:
appendFormat:
time
initWithValue:time:
_value
_time
removeAllObjects
hmf_removeFirstObject
indexOfObject:inSortedRange:options:usingComparator:
insertObject:atIndex:
hmf_objectsPassingTest:
indexesOfObjectsPassingTest:
objectsAtIndexes:
removeObjectsAtIndexes:
objectAtIndex:
na_map:
componentsJoinedByString:
initWithMaxCapacity:
objectsInTimeRange:includeEnd:
extractObjectsInTimeRange:
neighborsOfObject:
_lock
_maxCapacity
initWithTime:date:
date
_date
lastObject
dateAtTime:
timeIntervalSinceDate:
addDate:atTime:
timeIntervalSinceDateAtTime:
_buffer
addValue:
startedAtTime:
endedAtTime:
averageTime
_timeline
_average
initWithWindowSize:
addNumber:
movingAverage
movingAverageForInterval:defaultValue:
valueForInterval:defaultValue:
performBlock:
initWithUUID:homeUUID:
setMaxConcurrentOperationCount:
initWithTimeInterval:options:
resume
dictionary
initWithTimeout:
_updateSettings:
finish
addExecutionBlock:
operationQueue
addOperation:
watchdogTimer
taskServiceClient
homeUUID
objectForKeyedSubscript:
submitTaskWithOptions:completionHandler:
dataSource
initWithDataSource:faceCrop:
error
setCompletionBlock:
hmiPrivateErrorWithCode:
initWithDataSource:faceprint:
classifications
familiarity
sourceUUID
na_firstObjectPassingTest:
unknownFacesSavedCounts
sessionEntityUUID
intValue
numberWithInt:
setObject:forKeyedSubscript:
faceCrop
faceprint
storeUnassociatedFaceCrop:completion:
storeFaceprint:completion:
isPersonDataAvailableViaHomeKit
initWithSourceUUID:homeUUID:external:
suspend
analyticsTimer
settings
isFaceClassificationEnabled
standardUserDefaults
doubleForKey:
timeIntervalSinceReferenceDate
setDouble:forKey:
cancelWithError:
timerDidFire:
setDataSource:
handleUpdatedPerson:
handleUpdatedUnassociatedFaceCrop:
handleUpdatedPersonFaceCrop:
handleUpdatedFaceprint:
handleUpdatedSettings:
handleRemovedPersonWithUUID:
handleRemovedFaceCropWithUUID:
handleRemovedFaceprintWithUUID:
handleMisclassifiedPersonForFaceCrop:
handleNewFaceEvents:
lock
_dataSource
_settings
_operationQueue
_watchdogTimer
_analyticsTimer
_unknownFacesSavedCounts
setWithObject:
addFaceCrops:completion:
main
_faceCrop
_faceprint
initWithPixelBuffer:presentationTime:frameId:fragmentSequenceNumber:
jpegData
size
imageWithData:
extent
createPixelBufferWithSize:pixelFormat:useIOSurface:
contextWithOptions:
render:toCVPixelBuffer:
numberWithUnsignedLong:
dictionaryWithDictionary:
setObject:forKey:
appendBytes:length:
presentationTime
frameId
initWithPixelBuffer:
initWithJPEGData:presentationTime:frameId:fragmentSequenceNumber:size:
JPEGRepresentationWithDownscaleFactor:outSize:
fragmentSequenceNumber
motionDetections
setMotionDetections:
sessionPresentationTime
setSessionPresentationTime:
_frameId
_fragmentSequenceNumber
_jpegData
_motionDetections
_size
_presentationTime
_sessionPresentationTime
point
transferPixelBuffer:crop:size:pixelFormat:options:error:
numberWithFloat:
createPixelBufferFromJPEGDataProvider:error:
applyPadding:withOriginalSize:padding:
globalSession
releaseCachedResources
cropPixelBuffer:crop:error:
resizePixelBuffer:size:error:
resizePixelBuffer:size:pixelFormat:options:error:
createJPEGDataFromPixelBuffer:scale:encodeQuality:error:
createPixelBufferFromJPEGPath:error:
createPixelBufferFromJPEGData:error:
createPixelBufferFromImageData:error:
dewarpPixelBuffer:crop:size:pixelFormat:options:cameraModel:error:
imposeMinSizeFor:withOriginalSize:minCrop:
maintainAspectRatio:originalSize:ratioThreshold:
saveVideoFrame:videoId:baseURL:error:
transferPixelBuffer:rotationAngle:crop:size:precision:error:
releaseCachedVisionResources
initWithService:
readValue
_service
_updateThermalLevel
services
objectForKey:
readValueFromSensor:value:
readMaxValue:
thermalLevel
_client
_thermalLevelNotificationToken
_notificationQueue
_thermalLevel
_services
unsignedLongLongValue
tick
numberWithUnsignedLongLong:
setZeroPadsFractionDigits:
setAllowedUnits:
setCountStyle:
setAllowsNonnumericFormatting:
stringFromByteCount:
highWaterMark
initWithName:reason:userInfo:
stop
setHighWaterMark:
average
_highWaterMark
_tick
_updatePBSSystemLoad
pbsSystemLoad
isIdle
_pbsSystemLoadNotificationToken
_pbsSystemLoad
initWithAsset:readVideoTrack:readAudioTrack:
assetReaderWithAsset:error:
_createOutputsForAsset:readVideo:readAudio:
tracksWithMediaType:
assetReaderTrackOutputWithTrack:outputSettings:
setAlwaysCopiesSampleData:
canAddOutput:
addOutput:
cancelReading
copyNextSampleBuffer
status
copyNextSampleBufferWithTrackIndexOutput:
startReading
_copyNextSampleBufferFromTrackOutput:
initWithAsset:
_asset
_assetReader
_trackSamples
_trackOutputs
initWithCachePolicy:
setName:
setup
homes
hmf_firstObjectWithUUID:
personManager
residentDevices
isCurrentDevice
na_any:
uuid
photosPersonManagerWithUUID:
accessories
cameraProfiles
isSetup
defaultPrivateConfiguration
setOptions:
cachePolicy
setCachePolicy:
setDiscretionary:
homeKitOperationQueue
setDelegateQueue:
setDidUpdateHomes:
initWithHomeMangerConfiguration:
dateWithTimeIntervalSinceNow:
_refreshBeforeDate:completionHandler:
isPrimary
initWithNoCaching
homePersonManagerForHomeUUID:
homeForHMPersonManagerUUID:
homePersonManagersForCurrentDevice
photosPersonManagerForHomeUUID:sourceUUID:
isCurrentDevicePrimaryResident
cameraProfileWithUUID:
homeWithCameraProfileUUID:
_setup
_homes
_homeKitOperationQueue
_cachePolicy
didUpdateHomes
homeManager:didUpdateAuthorizationStatus:
homeManagerDidUpdateHomes:
homeManagerDidUpdatePrimaryHome:
homeManager:didAddHome:
homeManager:didRemoveHome:
homeManager:didReceiveAddAccessoryRequest:
_didUpdateHomes
initWithName:deferred:
begin
signpostLog
beginDate
hasBegun
shouldSignpost
signpostIdentifier
identifier
_name
_beginDate
_signpostIdentifier
_identifier
getUUIDBytes:
dataWithBytesNoCopy:length:freeWhenDone:
getBytes:range:
input
featureValueWithMultiArray:
initWithInput:inputName:
setInput:
_input
modelWithContentsOfURL:error:
setObject:atIndexedSubscript:
dataScalerInputName
scalerModel
predictionFromFeatures:error:
dataScalerOutputName
svmInputName
svmOutputName
dictionaryValue
defaultModelPath
defaultDataScalerPath
initWithModelPath:dataScalerPath:error:
predict:output:error:
_scalerModel
_status
_error
handleVideoSampleBuffer:
handleAudioSampleBuffer:
exceptionWithName:reason:userInfo:
handleSampleBuffer:
flush
flushAsync
task
setTask:
_task
photosPersonManager
personFromHomePerson:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:
modelUUID
faceCropUUID
initWithUUID:data:modelUUID:faceCropUUID:
addOrUpdateFaceprints:completion:
initWithHMPhotosPersonManager:
setPhotosPersonManager:
_photosPersonManager
stringWithUTF8String:
mainBundle
localizedStringForKey:value:table:
errorWithDomain:code:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:
arrayByAddingObjectsFromArray:
copyWithZone:
_personUUID
events
allObjects
frame
na_filter:
na_each:
eventClasses
maxConfidenceEventForEventClass:
decodeObjectOfClasses:forKey:
initWithFrame:events:
maxConfidenceEvents
_frame
_events
URLWithString:
initWithURL:options:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
respondWithData:
finishLoading
loadValuesAsynchronouslyForKeys:completionHandler:
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
loadValuesSynchronously
initWithValue:
windowSize
queue
setMovingAverage:
setQueue:
_movingAverage
_queue
_windowSize
integerValue
analyzerEvents
na_flatMap:
_eventsFromAnalyzerEvents:
_annotationScoresFromAnalyzerEvents:
frameResults
confidenceThatEventOccurred:
aggregatedEvents
resultCode
lastSequenceNumber
analysisFPS
duration
creationDate
isEqualToArray:
posterFrames
videoFragment
timeToAnalyzeFragment
timeSinceFragmentWasSubmitted
numberWithInteger:
initWithEvents:posterFrames:frameResults:annotationScores:duration:creationDate:resultCode:lastSequenceNumber:
initWithPosterFrames:frameResults:duration:creationDate:resultCode:lastSequenceNumber:
aggregatedEventTypes
isEqual:excludeTime:
annotationScores
setDuration:
setCreationDate:
setResultCode:
setTimeToAnalyzeFragment:
setTimeSinceFragmentWasSubmitted:
setVideoFragment:
setAnalysisFPS:
_analysisFPS
_annotationScores
_posterFrames
_frameResults
_creationDate
_resultCode
_timeToAnalyzeFragment
_timeSinceFragmentWasSubmitted
_videoFragment
_lastSequenceNumber
_duration
initWithIdentifier:name:manufacturer:model:
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:transcodeFragment:camera:
getAnalysisServiceTypePreference
numberPreferenceForKey:defaultValue:withMap:
allocWithZone:
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:transcodeFragment:
posterFrameGenerationInterval
posterFrameHeight
maxFragmentAnalysisDuration
maxFragmentDuration
transcodeFragment
setTranscodeFragment:
camera
serviceType
setServiceType:
startingMediaIntegritySequenceNumber
setStartingMediaIntegritySequenceNumber:
useScheduler
setUseScheduler:
inMediaAnalysis
setInMediaAnalysis:
faceClassificationEnabled
setFaceClassificationEnabled:
currentSessionDuration
setCurrentSessionDuration:
setHomeUUID:
_transcodeFragment
_useScheduler
_inMediaAnalysis
_faceClassificationEnabled
_posterFrameGenerationInterval
_posterFrameHeight
_maxFragmentAnalysisDuration
_maxFragmentDuration
_camera
_serviceType
_startingMediaIntegritySequenceNumber
_homeUUID
_currentSessionDuration
confidenceThatEventOccurred:events:annotationScores:
initWithEvents:annotationScores:videoFrame:
videoFrame
_videoFrame
_detectionsFromAnalyzerEvents:
_faceClassificationsFromAnalyzerEvents:
face
level
initWithFrame:events:annotationScores:detections:regionOfInterest:faceClassifications:
initWithFrame:regionOfInterest:analyzerEvents:
detections
regionOfInterest
faceClassifications
_detections
_faceClassifications
_analyzerEvents
_regionOfInterest
initWithSequenceNumber:data:moovFragment:eventTypes:activityZones:
setUrl:
moovFragment
appendData:
sequenceNumber
stringByAppendingFormat:
absoluteString
initWithSequenceNumber:data:moovFragment:
initWithSequenceNumber:data:moovFragment:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:activityZones:url:
fragmentData
eventTypes
activityZones
_sequenceNumber
_moovFragment
_eventTypes
_activityZones
_url
totalRequests
setTotalRequests:
flag
predictions
setPredictions:
totalPredictions
setTotalPredictions:
lastRequestResult
isEqualToSet:
repetitions
setRepetitions:
totalRepetitions
setTotalRepetitions:
setLastRequestResult:
setLastRequestSignificantEvents:
analyzer
scheduler
activeAnalyzerCount
maxPredictions
minRepetitions
lastRequestSignificantEvents
fragment
attributes
assetDuration
initWithMinRepetitions:maxPredictions:analyzer:
addRequest:result:significantEvents:
shouldPredictRequest:
predictedSignificantEventsForRequest:
predictedResultForRequest:
reset
_minRepetitions
_maxPredictions
_predictions
_repetitions
_totalPredictions
_totalRepetitions
_totalRequests
_lastRequestResult
_lastRequestSignificantEvents
_analyzer
significantEventsInternal
setFlag:
initWithAssetData:error:
loadAttributesFromVideoFragment:
analysisSubmissionTime
firstObject
dimensions
initWithDimensions:codecType:realTime:error:
initWithVideoFormat:audioFormat:
firstSequenceNumber
setNextSequenceNumber:
setAllowRecovery:
maxVideoEncoderFrameRate
initWithFrameRate:
encoder
retimer
frameSampler
initWithGenerationFrequency:andPosterFrameHeight:
initWithInput:
startHandlingFrames
readerForVideoFragment:workQueue:logIdentifier:
maxAnalysisFPS
initWithResourceAttributes:sampleRate:
preAnalyze:
handleMotionDetection:sessionPTS:frameDimensions:sessionIdentifier:
setAssetWriterDidOutputInitializationSegment:
mutableCopy
initWithInitializationSegment:separableSegment:
setAssetWriterDidOutputSeparableSegment:
assetWriter
initWithSampleBuffer:
localizedStringFromDate:dateStyle:timeStyle:
averageBitRate
points
drawPolygonWithNormalizedPoints:
drawTextHeaderBar:
draw:
baseDecodeTimeStamp
audioSamples
posterFrameGenerator
videoFrameResults
timeSinceAnalysisStart
timeSinceAnalysisSubmission
analysisStartTime
encoder:didEncodeSampleBuffer:
encoder:didFailWithError:
retimer:didRetimeSampleBuffer:
frameSampler:didSampleFrame:
frameSampler:didDropFrame:
selector:maySelectFrame:
selector:didDetectMotion:atSessionPTS:frameDimensions:
initWithVideoFragment:analyzer:maxAnalysisFPS:
addSignificantEvent:
significantEvents
markForPrediction
shouldSkipAnalysis
shouldFailAnalysis
loadAttributes
startAnalysis
startEncodingSessionForAsset:error:
startPosterFrameGeneratorWithInterval:frameHeight:
startAssetReaderWithWorkQueue:logIdentifier:
startFrameSelector
finishEncoderSession
makeDidAnalyzeResult
makeDidNotAnalyzeResultWithResultCode:
cancel
frameSelector
assetReader
setVideoFrameResults:
phase
setPhase:
_analysisSubmissionTime
_analysisStartTime
_maxAnalysisFPS
_fragment
_attributes
_encoder
_retimer
_frameSampler
_audioSamples
_assetWriter
_posterFrameGenerator
_frameSelector
_videoFrameResults
_significantEventsInternal
_phase
_flag
weakObjectsPointerArray
setSystemResourceUsageMonitorUsageLevel:
systemResourceUsageMonitorUsageLevel
numberPreferenceForKey:defaultValue:
isProductTypeJ105
currentState
isPictureInPictureActive
isAudioAccessory
inFullBypassMode
numberWithUnsignedInteger:
analyzers
isActive
averageAnalysisTimeMovingAverage
averageTotalAnalysisTimeMovingAverage
averageTotalAnalysisTime
isProductTypeJ42
updateAnalysisFPS:
internalAnalyzers
hmf_addObject:
assertOwner
addPointer:
compact
setCount:
_compactInternalAnalyzers
isPaused
logState
processPendingRequests
history
maxConcurrentAnalyzers
setInBypassMode:
_markPendingRequestsWithFlag:
averageAnalysisTime
pendingRequests
stringByPaddingToLength:withString:startingAtIndex:
_outcomeCountsAsString
mediaIntegritySequenceNumber
_flagCountsAsString
inBypassMode
inErrorState
analysisInProgress
appendString:
systemResourceUsageDidChangeTo:
transcodingAnalyzerCount
requestDidEnd:outcome:
registerAnalyzer:
removeAllAnalyzers
setPaused:
systemResourceUsageMonitor
analysisFPSPreference
_paused
_maxConcurrentAnalyzers
_internalAnalyzers
_systemResourceUsageMonitor
_systemResourceUsageMonitorUsageLevel
_averageAnalysisTimeMovingAverage
_averageTotalAnalysisTimeMovingAverage
_analysisFPSPreference
qosMap
isProductTypeB238
_scheduleRequest:
sessionEnded
configuration
_handleError:request:
setLastRequestSubmissionTime:
internalPendingRequests
hmf_enqueue:
pendingRequestsCount
lastRequestSubmissionTime
timeIntervalSinceNow
setSessionEnded:
setRunRemotely:
_handleError:request:description:
skipSequentialMediaIntegrityCheck
nominalFrameRate
_handleDidNotAnalyzeRequest:resultCode:
_shouldContinueAnalyzingRequest:resultCode:
hmf_maybeDequeue
_analyzeRequest:
setAnalysisInProgress:
_checkRequest:
_predictRequest:
_analyzeRequestRemotely:retryOnConnectionInterruption:
_analyzeRequestLocally:
setMediaIntegritySequenceNumber:
homePersonManager
remoteAnalysisService
preferenceOverrides
analyzer:didFindSignificantEvent:inFragment:
code
_handleError:request:underlyingError:
_handleDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:withResult:error:
requestAnalysisForAssetData:withProperties:andCompletionHandler:
warmStartModel
warmStart
_willAnalyzeRequest:
_handleError:request:description:underlyingError:
_analyzeRequestFramesLocally:
startReading:
analyzer:willAnalyzeRequest:
_finishEncodingSessionForRequest:withResult:
_sendAnalyticsEventForRequest:outcome:result:error:
shouldUploadVideoAnalysisEvent
sendEventForFaceEvent:homePersonManagerUUID:camera:
_requestDidEnd:outcome:
_notifyDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:resultCode:error:
_notifyDidNotAnalyzeRequest:withResult:
hmiErrorWithCode:description:underlyingError:
_enterErrorState
_notifyDidFailAnalysisForRequest:withError:
analyzer:didAnalyzeFragment:withResult:
analyzer:didAnalyzeRequest:withResult:
analyzer:didNotAnalyzeFragment:withResult:
analyzer:didNotAnalyzeRequest:withResult:
analyzer:didFailAnalysisForFragment:withError:
analyzer:didFailAnalysisForRequest:withError:
setInErrorState:
readNextFrame:error:
willHandleFramesFromVideoResource:
handleVideoFrame:error:
isFinishedEarly
_analyzeRequestFrames:
_handleDidAnalyzeRequest:
frames
_analyzeVideoFrame:request:result:error:
analyze:targetEventTypes:enableFaceClassification:sessionIdentifier:homeUUID:error:
saveToJsonActivityZones:motionDetection:videoFragmentUrl:frameId:UUID:detectionID:zoneID:
saveActivityZoneresultsInJSON:result:videoFrame:motionDetection:
filterEvents:withActivityZones:motionDetection:insetPercentageInclusion:insetPercentageExclusion:
generateAndSubmitStats:filteredEvents:motionDetection:activityZones:
_analyzeFrame:request:error:
filterDetectedObjects:request:result:
shouldSaveVideoFramesToDisk
saveFaceClassifications:videoId:fragmentId:frameId:baseURL:error:
manufacturer
model
numberWithBool:
eventConfidenceThresholdsHigh
eventConfidenceThresholdsMedium
string
bundleWithIdentifier:
infoDictionary
queryVersionInformation
classHierarchyMap
initWithConfiguration:identifier:
analyzeFragment:
clearPendingFragments
setHomePersonManager:
externalPersonManagers
setExternalPersonManagers:
streamAnalyzer
currentRequest
setCurrentRequest:
setConfiguration:
setRemoteAnalysisService:
setSaveVideoFramesToDisk:
_flagCounts
_outcomeCounts
_skipSequentialMediaIntegrityCheck
_analysisInProgress
_inErrorState
_inBypassMode
_sessionEnded
_uploadVideoAnalysisEvent
_saveVideoFramesToDisk
_delegate
_homePersonManager
_externalPersonManagers
_internalPendingRequests
_lastRequestSubmissionTime
_history
_streamAnalyzer
_currentRequest
_scheduler
_mediaIntegritySequenceNumber
_configuration
_remoteAnalysisService
appendFloats:count:
unsignedIntegerValue
initWithFloats:count:
mutableFloats
mutableBytes
floats
subtract:
initWithDataTensor:
fillWithFloat:
appendArray:
l2Norm
floatArrayByAdding:
floatArrayBySubtracting:
mutableCopyWithZone:
initWithShape:dataType:error:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
faceprinter
createFacePixelBufferForFaceDetection:pixelBuffer:roll:error:
computeJunkScoreForPixelBuffer:
qualityPredictionFromSVMUsingDetectorConfidence:laplacian:yaw:boxSize:error:
createFaceprintForFacePixelBuffer:fastMode:error:
faceAttributes
facemaskCategory
label
predictPersonFromFaceObservation:homeUUID:error:
descriptorData
currentModelUUID
floatValue
classificationThresholdKnown
linkedEntityUUID
classificationThresholdUnknown
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:familiarity:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:faceQualityScore:sessionEntityAssignment:
classifyFaceEvent:pixelBuffer:fastMode:homeUUID:error:
initWithError:
faceQualityFilter
_faceprinter
_faceQualityFilter
_classificationThresholdKnown
_classificationThresholdUnknown
setFrameAnalyzer:
modelTimeoutPreference
hmf_zeroUUID
frameAnalyzer
kick
initWithThresholdWithLabels:metricWithLabels:thresholdDefault:metricDefault:
initWithMediumConfidenceThresholds:highConfidenceThresholds:nmsConfiguration:assetPath:error:
ensureFrameAnalyzerWithError:
personThresholdMedium
petThresholdMedium
vehicleThresholdMedium
personThresholdHigh
petThresholdHigh
vehicleThresholdHigh
faceThreshold
getConfidenceThresholdPreferenceForKey:defaultConfidenceThreshold:
eventConfidenceThresholdsMediumFromTrial
eventConfidenceThresholdsHighFromTrial
eventConfidenceFaceThresholdFromTrial
_frameAnalyzer
containsObject:
vectorWithString:
CGAffineTransformValue
valueWithBytes:objCType:
valueAtIndex:
initWithDictionary:forType:
initWithDictionary:
_type
attributeForKey:
filterWithName:withInputParameters:
expectedAttributeForKey:
setValue:forKey:
outputImage
applyToImage:withProbability:
probability
_probability
imageData
initWithImageData:regionOfInterest:detections:
createRegionOfInterestPixelBufferWithError:
arrayWithArray:
initWithData:type:shape:strides:
distantPast
initWithBundleIdentifier:
isPermitted
saveRecordWithData:recordInfo:completion:
saveDESRecordWithVideoFrame:recordInfo:
augmentWithOptions:
createImageTensorWithError:
createBoxesTensorWithError:
createClassesTensorWithError:
createWeightsTensorWithError:
_boxesTensorData
_weightsTensorData
_classesTensorData
_imageData
samples
imageName
boxesName
weightsName
classesName
initWithSamples:imageName:boxesName:weightsName:classesName:
dataPointAtIndex:error:
numberOfDataPoints
_samples
_imageName
_boxesName
_weightsName
_classesName
initWithCVPixelBuffer:imageParameters:error:
setMaxNumberOfElements:
thresholdWithLabels
thresholdDefault
metricWithLabels
metricDefault
thresholdForLabel:
metricForLabel:
_thresholdWithLabels
_metricWithLabels
_thresholdDefault
_metricDefault
_labelIndex
_confidence
_boundingBox
sortedArrayUsingComparator:
nonMaximumSuppression:output:withThreshold:withMetric:
intersectionOverUnion:b:
intersectionOverMinArea:b:
convertObjectDetections:cropRect:originalImageSize:
presentationTimeStamp
recognizeFaces
sessionIdentifier
frameAnalyzer:didAnalyzeFrame:error:
handleSampleBuffer:motionDetections:
setRecognizeFaces:
_analysisTime
_recognizeFaces
_sessionIdentifier
frameAnalyzerDidAnalyzeFrame
setFrameAnalyzerDidAnalyzeFrame:
_frameAnalyzerDidAnalyzeFrame
initWithData:timeOffset:width:height:
timeOffset
width
height
_width
_height
_timeOffset
generationFrequency
frameHeight
_frameHeight
_generationFrequency
posterFramesInternal
nextGenerationTime
setNextGenerationTime:
saveAsPosterFrame:error:
setPosterFramesInternal:
_posterFramesInternal
_nextGenerationTime
detectFacesInImageData:error:
unalignedBoundingBox
faceBoxFromPhotosFaceCropImageData:
newDictionaryPopulatedWithFaceCropDataFromImageData:
dataUsingEncoding:
JSONObjectWithData:options:error:
isEqualToData:
isEqualToDate:
encodeRect:forKey:
decodeRectForKey:
faceCropFromPhotosFaceCropImageData:
_UUID
_dataRepresentation
_dateCreated
_faceBoundingBox
_lastSample
retimerDidRetimeSampleBuffer
setRetimerDidRetimeSampleBuffer:
_retimerDidRetimeSampleBuffer
_manufacturer
_model
initWithTaskID:timeout:
initWithTaskID:
results
taskID
_taskID
initWithTaskID:homeUUID:timeout:
initPrivate
nextTaskID
setNextTaskID:
buildUpdatePersonsModelTaskFromOptions:error:
buildRemovePersonsModelTaskFromOptions:error:
buildHomePersonClusteringTaskFromOptions:error:
buildTuriTrialUpdateTaskFromOptions:error:
getNextTaskID
buildFaceMisclassificationTaskFromOptions:error:
buildPersonsModelsSummaryTaskFromOptions:error:
buildSubmitFeedbackTaskFromOptions:error:
submitTask:completionHander:
operations
enumerateObjectsUsingBlock:
boolValue
stringPreferenceForKey:defaultValue:
initWithHMHomePersonManager:
initWithHomeUUID:sourceUUID:error:
initWithTaskID:homeUUID:sourceUUID:dataSource:externalLibrary:removeExcessFaceCrops:
initWithTaskID:homeUUID:sourceUUID:
initWithTaskID:dataSource:faceCrop:
initWithTaskID:homeUUID:dataSource:sourceUUID:personsModelManager:error:
initWithTaskID:homeUUID:
initWithTaskID:cameraProfileUUID:clipUUID:
cancelTask:
_nextTaskID
taskService
allowedClasses
privateDescription
propertyDescription
initWithDataSource:
persons
_persons
score
initWithFrame:score:
_score
hasPreferenceForKey:
initWithResourceAttributes:sampleRate:targetInterval:
valuePreferenceForKey:defaultValue:withMap:
alloc
initWithSize:
framesInternal
detector
appendFramePixelBuffer:atTime:
detectWithGlobalMotionScore:
sortUsingComparator:
maxFrameCount
predictedFrames
removeObject:
sampler
appendFrame:error:
sampler:didFindSample:
sampler:didFindSample:target:
sampler:didDiscardFrame:
sampleRate
_sampler
_framesInternal
_maxFrameCount
_predictedFrames
_detector
_sampleRate
setFrame:
sampleInterval
targetInterval
unmatchedSampleFrames
isMarkedAsFinished
_appendFrame:error:
setMarkedAsFinished:
_markedAsFinished
_unmatchedSampleFrames
_targetInterval
_sampleInterval
existingAtCurrentVersion
unionSet:
createdAtCurrentVersion
initWithExistingAtCurrentVersion:createdAtCurrentVersion:existingAtOtherVersions:
allAtCurrentVersion
existingAtOtherVersions
_existingAtOtherVersions
_createdAtCurrentVersion
_existingAtCurrentVersion
_minorVersionFromVisionVersion:
hmf_UUIDWithNamespace:data:
initWithCVPixelBuffer:options:
setInputFaceObservations:
setPrivateRevision:error:
setDetectionLevel:
setRevision:
performRequests:error:
setFaceprint:
createFacePixelBufferFromFaceCrop:error:
generateFaceprintForFaceCrop:error:
updatedFaceprintsForFaceCrops:withExistingFaceprints:error:
initWithData:options:
detectFacesInPixelBuffer:error:
_modelUUID
_faceCropUUID
initWithPoint:
_point
encodeBool:forKey:
decodeBoolForKey:
fileExistsAtPath:
fileHandleForReadingAtPath:
assetDirectoryPath
unpackageModelAssets:intoDirectory:error:
removeItemAtPath:error:
unTarFileWithFd:toPath:
unpackageModelAssetsAtPath:error:
pendingUpdates
setPendingUpdates:
_pendingUpdates
defaultSessionConfiguration
sessionWithConfiguration:delegate:delegateQueue:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
homeKitClient
session
feedbackServiceHost
_homeKitClient
_session
_feedbackServiceHost
protectionSpace
host
serverTrust
credentialForTrust:
authenticationMethod
feedbackSession
_temporaryFileURLWithUUID:extension:error:
clipManager
initWithClipManager:clip:
setClipDestinationFileURL:
hmiPrivateErrorWithCode:description:suggestion:underlyingError:
setFetchVideoAssetContextCompletionBlock:
setDownloadProgressHandler:
faceClassification
person
setFaceCrops:
fetchClipWithUUID:completion:
assetWithURL:
composition
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
initWithAsset:presetName:
setOutputFileType:
setOutputURL:
setShouldOptimizeForNetworkUse:
isInternalInstall
setTimeRange:
exportAsynchronouslyWithCompletionHandler:
feedbackServiceURL
base64EncodedDataWithOptions:
initWithData:encoding:
stringByAppendingPathComponent:
requestWithURL:
setHTTPMethod:
setValue:forHTTPHeaderField:
uploadTaskWithRequest:fromFile:completionHandler:
dataWithLength:
faceCrops
_base64StringFromData:
_attachEncryptedDataUsingKey:toPayload:error:
dataWithJSONObject:options:error:
dataWithContentsOfURL:
setAssetData:
serviceResult
_createPayloadWithServiceResult:error:
_uploadPayloadData:uploadURL:completionHandler:
_stripAudioTrackFromAsset:completionHandler:
setServiceResult:
_downloadClipWithCameraProfileUUID:clipUUID:completionHandler:
_requestPreSignedURLWithClipUUID:completionHandler:
feedbackRequestURLForClipWithUUID:
dataTaskWithURL:completionHandler:
removeItemAtURL:error:
_removeTemporaryFiles
_submitClipWithCameraProfileUUID:clipUUID:completionHandler:
initWithFeedbackSession:cameraProfileUUID:clipUUID:
_attachFaceCrops:toPayload:error:
cameraProfileUUID
clipUUID
temporaryFileURLs
assetData
_feedbackSession
_cameraProfileUUID
_clipUUID
_temporaryFileURLs
_faceCrops
_assetData
_serviceResult
_operation
submitFeedbackWithCameraProfileUUID:clipUUID:runRemotely:completionHandler:
submitFeedbackWithCameraProfileUUID:clipUUID:completionHandler:
initWithPersonUUID:sourceUUID:sessionEntityUUID:confidence:familiarity:
initWithUUID:sourceUUID:faceBoundingBox:
initWithUUID:sourceUUID:sessionEntityUUID:faceBoundingBox:facecrop:faceprint:confidence:
decodeDoubleForKey:
decodeIntegerForKey:
encodeDouble:forKey:
encodeInteger:forKey:
initWithUUID:name:personsModelIdentifier:faceBoundingBox:
personsModelIdentifier
_personsModelIdentifier
_sourceUUID
_sessionEntityUUID
_familiarity
summaryForHomeUUID:error:
sendEventForPersonsModels:
removeNearestFaceprint:withFaceCrops:
hmf_isEmpty
faceDistanceFromDescriptor:toDescriptor:
removeFaceCropsWithUUIDs:completion:
removePersonsModelForHomeUUID:sourceUUID:error:
initWithUUID:
supportsFaceClassification
setSupportsFaceClassification:
setPersonDataAvailableViaHomeKit:
_supportsFaceClassification
_personDataAvailableViaHomeKit
levelThresholds
characterAtIndex:
initWithValue:levelThresholds:
_levelThresholds
_hmiErrorWithCode:description:reason:suggestion:underlyingError:
hmiErrorWithCode:underlyingError:
hmiErrorWithCode:
hmiErrorWithCode:description:
initWithData:timeRange:
initWithInitializationSegment:separableSegment:timeRange:
numberWithUnsignedInt:
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:
_ensureAttributes
videoFormatDescription
audioFormatDescription
initializationSegment
separableSegment
isCombinableWithFragment:
canFragmentData:
dataWithData:
timeRange
videoTrackTimeRange
sequenceNumbers
decodeCMTimeRangeForKey:
encodeCMTimeRange:forKey:
isInitializationSegment:combinableWithInitializationSegment:
fragmentData:handler:
initWithInitializationSegment:separableSegment:sequenceNumbers:
audioTrackTimeRange
_attributesLoaded
_videoFormatDescription
_audioFormatDescription
_initializationSegment
_separableSegment
_sequenceNumbers
_videoTrackTimeRange
_audioTrackTimeRange
_timeRange
initWithFrameDimensions:
handleMotionDetections:atTime:
sessionEntities
allKeys
trackNewFaces:personWithoutFaceEvents:personWithFilteredFaceEvents:withMotionDetection:atTime:
getUUIDToNextFaceIndexWithPreviousFaceindex:
predictedLinkedEntityUUIDs
intersectsSet:
removeObjectAtIndex:
faceQualityScore
hasMotionVectors
initWithConfidence:boundingBox:hasMotionVectors:face:
handleMotionDetection:sessionPTS:
assignSessionEntitiesToPersonWithFaceEvents:personWithoutFaceEvents:personWithFilteredFaceEvents:videoFrame:
_faceTracker
_sessionUUIDToPreviousFaceIndex
_sessionUUIDToPreviousFaceprints
_sessionEntities
configure
loadModelFromTrialWithError:
_createFontWithSize:
dictionaryWithObjectsAndKeys:
initWithString:attributes:
_sbuf
_context
_colorSpace
_font
resourceUsageMonitor
_resourceUsageMonitor
initWith
applyWithFrameResult:
initWithActivityZones:
personManagerUUID
initWithPersonUUID:sourceUUID:
personSourceUUIDPairFromPersonLink:
initWithUUID:name:personLinks:
defaultFormatter
initWithName:value:options:formatter:
personLinks
initWithUUID:name:
_personLinks
initWithSourceUUID:externalLibrary:faceCountsByPerson:
isExternalLibrary
faceCountsByPerson
_externalLibrary
_faceCountsByPerson
visionPersonsModel
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
initWithPersonsModel:homeUUID:sourceUUID:externalLibrary:
summary
_visionPersonsModel
_addValueToContainer:forKey:
_containerIsArray
null
_valueForNumber:
stringFromDate:
base64EncodedStringWithOptions:
_JSONObjectWithObject:options:
na_dictionaryByMappingValues:
initWithDictionary
_addClassToContainer:
object
numberWithLongLong:
decimalNumberWithString:
JSONObjectStringWithObject:pretty:options:
JSONObjectWithObject:options:
JSONObjectStringWithObject:
allowsKeyedCoding
initWithArray
encodeInt32:forKey:
encodeInt64:forKey:
objectJSON
objectPrettyJSON
options
_container
_options
hasPrefix:
hasSuffix:
_objectWithJSONObject:allowedClasses:
classMap
initWithJSONObject:
setClassMap:
objectWithJSONData:classMap:
objectWithJSONObject:classMap:
objectWithJSONObjectString:classMap:
decodeInt32ForKey:
decodeInt64ForKey:
container
_classMap
timeZoneForSecondsFromGMT:
setTimeZone:
initWithLocaleIdentifier:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
initWithData:elementCount:elementType:lengthInBytes:labelsAndConfidence:requestRevision:
setFaceId:
anyObject
initWithFaceEvent:
URLByAppendingPathExtension:
writeToURL:atomically:encoding:error:
faceObservationsFromFaceprintsForClustering:
faceObservationFromFaceprint:
estimatePersonBoundingBoxFromFaceBoundingBox:
eventsWithPersonsAndFacesMergedFromEvents:
boundingBoxForFaceTracker
_computeBlobPropertiesWithBoundingBox:personBoundingBox:dx:dy:motionVectors:isDetection:
blobID
personBoundingBox
motionMean
frameDimensions
boxDistanceToPersonBlob:
sizeDistanceToPersonBlob:
isBijectiveToPersonBlob:
isMoving
setFaceBoundingBox:
setPersonBoundingBox:
personIndices
personIndex
personIouMax
origin
target
midpoint
motion
setMotionMean:
setPosition:
initWithPersonWithFaceEvent:motionVectors:personIndex:regionOfInterest:frameDimensions:
initWithPersonWithoutFaceEvent:personIndex:regionOfInterest:frameDimensions:
initWithPersonBlob:motionVectors:personIndex:regionOfInterest:frameDimensions:
similarityToPersonBlob:
adjustFaceBoundingBoxFromPersonBlob:
isLost
setPersonIndex:
setPersonIndices:
setPersonIouMax:
_personIouMax
_personIndex
_personIndices
_blobID
_frameDimensions
_motionMean
_personBoundingBox
initWithProjectedFaceIndex:detectedFaceIndex:score:
projectedFaceIndex
detectedFaceIndex
_projectedFaceIndex
_detectedFaceIndex
compare:
initWithMotionVectors:regionOfInterest:time:
motionVectors
_motionVectors
setPreviousTime:
motionRecordsQueue
previousPersons
setPreviousPersons:
setPreviousProjectedPersonIndices:
setPreviousFilteredPersonIndices:
previousTime
indexSetWithIndexesInRange:
containsIndex:
removeIndex:
removeObjectForKey:
enumerateKeysAndObjectsUsingBlock:
enumerateIndexesUsingBlock:
subarrayWithRange:
previousProjectedPersonIndices
previousFilteredPersonIndices
_previousPersons
_previousProjectedPersonIndices
_previousFilteredPersonIndices
_motionRecordsQueue
_previousTime
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:nominalFrameRate:dimensions:baseDecodeTimeStamp:
initWithAssetDuration:creationDate:
_firstSequenceNumber
_nominalFrameRate
_dimensions
_assetDuration
_baseDecodeTimeStamp
initWithVideoFragment:workQueue:logIdentifier:
resourceLoaderWorkQueue
initWithAsset:error:
setAssetReader:
assetKeys
_propertiesLoadedForAsset:resultCallback:
fragments
statusOfValueForKey:error:
_didKeyValueLoadFailed:
trackKeys
_propertiesLoadedForTrack:fromAsset:resultCallback:
initWithTrack:outputSettings:
setMaximizePowerEfficiency:
_validateSequentialIntegrityOfFragmentsInAsset:
_sequenceNumberOfLastFragmentInAsset:
_sequenceNumberOfFirstFragmentInAsset:
dateValue
formatDescriptions
latentBaseDecodeTimeStampOfFirstTrackFragment
outputs
currentFrameId
setCurrentFrameId:
request
finishLoadingWithError:
requestsAllDataToEndOfResource
_logIdentifier
_currentFrameId
_resourceLoaderWorkQueue
_initSessionWithError:
setMaxKeyFrameIntervalDuration:
setAverageBitRate:
_invalidateSession
_encodeSampleBuffer:attemptRecovery:
maxKeyFrameIntervalDuration
numberOfDroppedFrames
_forceKeyFrameOnNextEncodedFrame
_codecType
_realTime
_maxKeyFrameIntervalDuration
_averageBitRate
_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
encoderDidFailWithError
setEncoderDidEncodeSampleBuffer:
setEncoderDidFailWithError:
_encoderDidEncodeSampleBuffer
_encoderDidFailWithError
addOrUpdateFaceCrops:completion:
addOrUpdatePersons:completion:
fetchAllUnassociatedFaceCropsWithCompletion:
removePersonsWithUUIDs:completion:
associateFaceCropsWithUUIDs:toPersonWithUUID:completion:
addPersons:completion:
isImportingFromPhotoLibraryEnabled
isSharingFaceClassificationsEnabled
setImportingFromPhotoLibraryEnabled:
setSharingFaceClassificationsEnabled:
_importingFromPhotoLibraryEnabled
_sharingFaceClassificationsEnabled
sessionEntityAssignment
numberOfFaceprintsClustered
numberOfClusters
numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
modelSummaries
bucketForValue:usingBuckets:
valueForKeyPath:
homeToExternalEquivalencies
externalToExternalEquivalencies
sendEventForClusteringTask:
initWithModelSummaries:homeToExternalEquivalencies:externalToExternalEquivalencies:
_modelSummaries
_homeToExternalEquivalencies
_externalToExternalEquivalencies
initWithSourceUUID:personUUID:confidence:linkedEntityUUID:
_linkedEntityUUID
initWithPersonsModels:userDefinedPersonLinks:error:
personsModelsByHome
setMaximumIdentities:
initWithConfiguration:
userDefinedPersonLinksByHome
isEqualToDictionary:
persistUserDefinedPersonLinks:forHomeUUID:error:
addFaceObservations:toPersonWithUniqueIdentifier:error:
loadModelsWithError:
homePersonsModelForHomeWithUUID:
getModelStoragePathForModel:error:
persistModel:toPath:error:
buildEquivalencyMapForPersonsModels:userDefinedPersonLinks:error:
equivalencyTablesByHome
getUserDefinedPersonLinksStoragePathForHomeUUID:error:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
initWithUUIDString:
equivalencyCellForPerson:
modelFromURL:options:error:
URLByDeletingLastPathComponent
setReadOnly:
writeToURL:options:error:
getRootModelStoragePathWithError:
pathWithComponents:
getModelStoragePathForHome:error:
URLByAppendingPathComponent:isDirectory:
absoluteURL
archivedDataWithRootObject:requiringSecureCoding:error:
fileHandleForReadingFromURL:error:
readDataToEndOfFile
unarchivedObjectOfClasses:fromData:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
regularExpressionWithPattern:options:error:
lastPathComponent
numberOfMatchesInString:options:range:
modelURLsFromPath:error:
loadPersonsModelFromURL:externalLibrary:homeUUID:error:
loadUserDefinedPersonLinksForHomeUUID:error:
URLByDeletingPathExtension
pathExtension
loadModelAtPath:error:
personToEquivalencyCell
buildPersonsModelForHomeUUID:sourceUUID:externalLibrary:faceObservationsByPerson:error:
predictHomePersonFromFaceObservation:homeUUID:error:
equivalencyCellForPerson:homeUUID:error:
_userDefinedPersonLinksByHome
_personsModelsByHome
_equivalencyTablesByHome
initWithName:weights:biases:
weights
biases
_weights
_biases
initWithLayerParameters:losses:
layerParameters
losses
_layerParameters
_losses
getParameterOfType:forLayerNamed:error:
networkPath
initWithTrainingNetworkPath:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:trainingControlVariableName:withInitializer:error:
initWithTrainingModelDefinition:forPlatform:error:
getParametersFromLayers:fromTask:error:
getTensorNamed:
doTrainingOnData:forNumberOfEpochs:withCallback:error:
initWithTrainingNetworkPath:data:error:
trainLayers:epochs:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:error:
_networkPath
initWithCode:message:
message
success
skipped
isSkipped
isSuccess
_message
_code
removePersonsModelWithRetryOnError:
external
_external
_getAnalyzers
_updateAnalyzer:withIndex:
_logState
monitored
delay
stringWithString:
stateDescription
_usageMonitor
_usageLevel
clientWithIdentifier:
updateLevels
submitUpdateModelTask
addUpdateHandlerForNamespaceName:usingBlock:
levelForFactor:withNamespaceName:
fileValue
registerForTrialUpdates
modelPath
_trialClient
_compiledModelArchivePath
_personThresholdHigh
_personThresholdMedium
_petThresholdHigh
_petThresholdMedium
_vehicleThresholdHigh
_vehicleThresholdMedium
_faceThreshold
_modelPath
lengthInBytes
faceId
initWithFaceThreshold:singleLinkThreshold:percentConnectionsThreshold:faceprintRevision:error:
addFaceObservations:toFaceDescriptorBuffer:error:
convertToClusters:
setObjects:
setClusterId:
objects
setTotalObjectCount:
setShouldUpdateRepresentative:
dataWithBytes:length:
centermostFaceprintInCluster:faceObservations:
getClustersWithFaces:error:
.cxx_construct
_greedyClusterer
initWithConfiguration:identifier:remote:
thumbnailInterval
thumbnailHeight
transcode
eventTriggers
legacyAnalyzer
_handleFragment:withResult:events:outcome:
analyzer:didFailWithError:
initWithJPEGData:size:presentationTimeStamp:
_makeFrameResult:withPresentationTimeStamp:
analyzer:didAnalyzeFrame:
initWithFragment:events:frameResults:thumbnails:configuration:outcome:
analyzer:didAnalyzeFragment:
analyzeFragment:configuration:
setMonitored:
_group
_configurationBySequenceNumber
_pendingFragments
_failed
_cancelled
_sessionDuration
_legacyAnalyzer
initWithInterval:
_interval
_firstPTS
_lastIndex
frameSamplerDidSampleFrame
frameSamplerDidDropFrame
setFrameSamplerDidSampleFrame:
setFrameSamplerDidDropFrame:
_frameSamplerDidSampleFrame
_frameSamplerDidDropFrame
packageAnalyzerDidDetectPackage
packageAnalyzer:didDetectPackage:error:
setPackageAnalyzerDidDetectPackage:
_packageAnalyzerDidDetectPackage
_numBins
_maxLaplacianScore
_minLaplacianScore
_binWidth
_maxScore
_histogram
computeJunkScoreForFacePrint:
_maxDistanceScore
_junkCentroid
cutOffThresholds
maximumDifferences
computeScoreWithYaw:laplacian:detectorConfidence:boxSize:
_cutOffThresholds
_maximumDifferences
thumbnails
outcome
_thumbnails
_outcome
decodeCMTimeForKey:
encodeCMTime:forKey:
decodeIntForKey:
encodeInt:forKey:
initWithPoints:isInclusion:
isInclusion
overlapsWithElipseInsideRect:
overlapsWithElipseInsideRect:withInsetPercentage:
checkIfObjectIsStaticWithBoundingBox:motionDetection:eventType:
containsCornersOfRect:withInsetPercentage:
classShortNameStringMap
stringByDeletingPathExtension
jsonReperesentaionOfDetectedObject:motionDetection:eventType:
na_all:
submitCoreAnalyticsEvent:filteringLevel:numberOfDetectedObjects:
initWithPoints:
_inclusion
_points
minFrameQuality
minFrameScale
setThumbnailInterval:
setThumbnailHeight:
setMaxFragmentAnalysisDuration:
setMaxFragmentDuration:
setTranscode:
setMinFrameQuality:
setMinFrameScale:
setCamera:
_transcode
_minFrameQuality
_minFrameScale
_thumbnailHeight
_thumbnailInterval
detectPackages
setActivityZones:
setEventTriggers:
setDetectPackages:
_detectPackages
_eventTriggers
raise:format:
initWithBoundingBox:
applyEventTypeAndCheckIfSubBoundingIsStatic:forMetric:eventType:confidence:
_finish
didAnalyzeFragment
didFailAnalysisForFragment
didFindSignificantEvent
didNotAnalyzeFragment
setDidAnalyzeFragment:
setDidFailAnalysisForFragment:
setDidNotAnalyzeFragment:
setDidFindSignificantEvent:
_didAnalyzeFragment
_didFailAnalysisForFragment
_didFindSignificantEvent
_didNotAnalyzeFragment
strongToWeakObjectsMapTable
nextRequestID
setNextRequestID:
runRemotely
setPreferenceOverrideFromDictionary:
getNextRequestID
requests
expectedClasses
requestAnalysisForPixelBuffer:withProperties:andCompletionHandler:
cancelRequest:
_runRemotely
_nextRequestID
_requests
initWithConfidence:boundingBox:hasMotionVectors:
hasEstimatedBoundingBox
initWithConfidence:boundingBox:face:
_isBoundingBoxEstimated
_face
flattenTrainingResult:
encryptedDataWithPublicKey:inPlaceDataFloatNumbers:count:error:
packageTrainingResult:privatize:maxNorm:normBinCount:encryptionKey:error:
preferenceCache
preferenceOverridesInternal
addEntriesFromDictionary:
preferenceLoggedValues
initWithKey:options:domain:defaultValue:
systemPreferenceValueForKey:
logPreferenceForKey:value:
caseInsensitiveCompare:
addPreferenceOverrideFromDictionary:
removeAllPreferenceOverrides
numberPreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withParser:
preferenceCacheFlushTimer
_preferenceCacheFlushTimer
_preferenceCache
_preferenceLoggedValues
_preferenceOverridesInternal
setNumberStyle:
numberFromString:
initWithDataSource:person:
personFaceCrops
_person
_personFaceCrops
token
notificationName
publishValueForToken:
publishInitialValue
setToken:
callback
initWithNotificationName:andQueue:andCallback:
_token
_notificationName
_callback
setMaximumFractionDigits:
stringFromNumber:
URLForDirectory:inDomain:appropriateForURL:create:error:
setNumberOfFaceprintsClustered:
setNumberOfClusters:
setNumberOfPersonsCreated:
setNumberOfUnknownFaceprintsAssociated:
setFaceprintingDuration:
setClusteringDuration:
setTotalDuration:
setError:
_numberOfFaceprintsClustered
_numberOfClusters
_numberOfPersonsCreated
_numberOfUnknownFaceprintsAssociated
_faceprintingDuration
_clusteringDuration
_totalDuration
_stageZero_fetchFaceCrops
_stageOne_fetchFaceprints:
_stageTwo_generateFaceprintsForFaceCrops:existingFaceprints:
_stageThree_clusterFaceprints:
personsModelManager
_stageFour_addPersons:clusterMapping:faceprints:
_stageFive_associateFaceCropsWithClusterMapping:faceprints:
_stageSix_expireUnnamedPersons
isCancelled
hmf_isEqualToUUID:
personCreatedDateFromFaceCrops:
startTime
_clusterer
_faceClassifier
_personsModelManager
_summary
_startTime
waitUntilFinished
initWithDataSource:faceCropUUIDs:
faceprints
initWithDataSource:faceprints:
initWithDataSource:faceprintUUIDs:
shouldRemoveExcessFaceCrops
subsampleFacesForPersons:withFaceObservationsMap:dataSource:vnUUIDToFaceCropUUIDMap:
trainingFaceObservationsForPersonWithUniqueIdentifier:canceller:error:
na_setByRemovingObjectsFromSet:
_removeExcessFaceCrops
faceCropUUIDs
_faceCropUUIDs
_faceprints
initWithFrame:size:
cropRect
_cropRect
mapTableWithKeyOptions:valueOptions:
setCountLimit:
significantActivityDetector
regionOfInterestOperations
regionOfInterestOperationQueue
sessionEntityManagers
_analyzerEventsFromObjectDetections:
_simulatedEventForEventClass:
_targetEventsSetFromTargetEventTypes:enableFaceClassification:
_filterEvents:targetEventClasses:
_filterEvents:withMotionDetections:cropRectNormalized:
_eventsWithFaceClassificationAppliedFromEvents:videoFrame:sessionIdentifier:homeUUID:error:
eventsWithContentsOfFile:
compressedFrameWithScale:quality:error:
saveDESRecordVideoFrame:withResult:
lowercaseString
highConfidenceThresholds
mediumConfidenceThresholds
_rankForEventClass:
_createStationaryEventFromEvent:
confidenceLevel
faceClassifier
transaction
setTransaction:
_mediumConfidenceThresholds
_highConfidenceThresholds
_regionOfInterestOperationQueue
_regionOfInterestOperations
_significantActivityDetector
_transaction
_sessionEntityManagers
taskIdentifier
taskRunnerClass
initWithURL:
activityForScheduling
pixelBufferFrameWithError:
printWithHeight:
_store
_presentationTimeStamp
initWithContentsOfFile:options:error:
fileDescriptor
availableData
unpackageTarData:size:parentDir:
getDataOutWithSize:withFlag:fd:
uncompressedContentsForCompressedFile:outPath:
buffer:willHandleSampleBuffer:
bufferWillFlush:
bufferWillFinish:
initWithMaxDuration:
handleBlock:
videoSampleCount
videoDuration
videoDelay
_semaphore
_count
_maxDuration
_delay
bufferWillHandleSampleBuffer
bufferWillFinish
bufferWillFlush
setBufferWillHandleSampleBuffer:
setBufferWillFlush:
setBufferWillFinish:
_bufferWillHandleSampleBuffer
_bufferWillFlush
_bufferWillFinish
dataWithContentsOfFile:options:error:
_hasMotionVectors
_userInfo
_drainBuffer:
_failWithDescription:
decoder:didDecodeSampleBuffer:
_invalidateSession:
handleSampleBuffer:outputFrame:
_createSessionWithFormatDescription:
decoder:didFailWithError:
_didDecodeSampleBuffer:
_reorderBufferDidBecomeFull
_lastSampleBufferPTS
_lastSampleBufferDTS
decoderDidDecodeSampleBuffer
decoderDidFailWithError
setDecoderDidDecodeSampleBuffer:
setDecoderDidFailWithError:
_decoderDidDecodeSampleBuffer
_decoderDidFailWithError
_checkNotStarted
typeWithIdentifier:
initWithContentType:
setOutputFileTypeProfile:
setPreferredOutputSegmentInterval:
setInitialSegmentStartTime:
setInitialMovieFragmentSequenceNumber:
setProducesCombinableFragments:
initWithMediaType:outputSettings:sourceFormatHint:
setExpectsMediaDataInRealTime:
setMediaTimeScale:
canAddInput:
addInput:
assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
startWriting
assetWriter:didOutputInitializationSegment:
trackReports
mediaType
firstVideoSampleInformation
assetWriter:didOutputSeparableSegment:timeRange:
_appendSampleBuffer:
_createAssetWriterWithInitialSegmentStartTime:
startSessionAtSourceTime:
allowRecovery
isReadyForMoreMediaData
_removeTrimDurationAttachmentsFromAudioSampleBuffer:
allowRecoveryFromInsufficientAudioTrim
_ensureAudioSampleBufferHasSufficientPrimingTrim:
appendSampleBuffer:
flushSegment
assetWriter:didFailWithError:
assetWriter:didOutputSegmentData:segmentType:segmentReport:
assetWriter:didOutputSegmentData:segmentType:
nextSequenceNumber
setAllowRecoveryFromInsufficientAudioTrim:
_skipInitializationSegment
_dropSamplesUntilSync
_dropTrimDurationAttachments
_outputQueue
_videoFormat
_audioFormat
_allowRecovery
_allowRecoveryFromInsufficientAudioTrim
_nextSequenceNumber
assetWriterDidOutputInitializationSegment
assetWriterDidOutputSeparableSegment
assetWriterDidFailWithError
setAssetWriterDidFailWithError:
_assetWriterDidOutputInitializationSegment
_assetWriterDidOutputSeparableSegment
_assetWriterDidFailWithError
inputs
faceObservationsForPersonWithUniqueIdentifier:error:
setByAddingObject:
facesAreSamePersonFromSet:andSet:
setByAddingObjectsFromSet:
facesAreSamePersonFromSet:andSet:distanceThreshold:percentMatchingThreshold:
_personToEquivalencyCell
updatePersonsModelWithRetryOnError:
initWithOrigin:motion:
distance
eventType
setEventType:
_eventType
_origin
_motion
initWithPixelBuffer:atTime:
classMotionScoreMap
classPaddingMap
scoreForSubBoundingBox:forMetric:eventType:confidence:
initWithBoundingBox:size:motionVectors:
_computeOpticalFlow:with:globalMotionScore:
_frames
sbuf
initWithSampleBuffer:score:detections:
setSampleRate:
_handleReference:target:
_drainCandidatesThatExpiredBefore:
_ensureDetectorForPixelBuffer:
frameSelector:didSelectFrame:detections:
_candidates
_enabled
_referenceInterval
_expirationInterval
_reference
frameSelectorDidSelectFrame
setFrameSelectorDidSelectFrame:
_frameSelectorDidSelectFrame
initWithFaceCrop:faceprint:classifications:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:
_classifications
_faceQualityScore
_predictedLinkedEntityUUIDs
_sessionEntityAssignment
analyzerWithConfiguration:identifier:legacy:remote:error:
handleAssetData:withOptions:completionHandler:
analyzerWithOptions:error:
analyzerWithConfiguration:identifier:error:
handleAssetData:withOptions:errorHandler:
handleMessageWithOptions:completionHandler:
_notifyDelegateDidFailWithError:
_ensureAssetWriterForFragment:
handleSampleBuffer:errorHandler:
_handleDecodedSampleBuffer:
_ensureEncoderForSampleBuffer:
dynamicConfigurationForTime:
_notifyDelegateDidAnalyzeFrame:
_notifyDelegateDidAnalyzeFragment:
_produceResult:withArguments:
analyzer:didProduceResult:
_commandBuffer
_decoder
_frameThumbnailSampler
_encode
_currentPTS
_currentDTS
_frameAnalyzerResultBuffer
_thumbnailBuffer
_dynamicConfigurationBuffer
_packageAnalyzer
_numDecodedSamples
_numDidAnalyzeFrames
_numDidAnalyzeFragments
_monitored
analyzerDidAnalyzeFrame
analyzerDidAnalyzeFragment
analyzerDidFailWithError
setAnalyzerDidAnalyzeFrame:
setAnalyzerDidAnalyzeFragment:
setAnalyzerDidFailWithError:
_analyzerDidAnalyzeFrame
_analyzerDidAnalyzeFragment
_analyzerDidFailWithError
getStoragePath
getResourceValue:forKey:error:
dataWithContentsOfURL:options:error:
_sourceURL
@16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v24@0:8@?16
v32@0:8@16@?24
v24@0:8@?<v@?@"NSSet"@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSSet"@"NSError">24
v24@0:8@?<v@?@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSError">24
v24@0:8@?<v@?@"HMIExternalPersonManagerSettings"@"NSError">16
@"HMFLogCategory"16@0:8
@24@0:8@16
v24@0:8@16
v16@0:8
@"NSMutableArray"
@"NSMutableSet"
@"HMIDESMutableFloatArray"
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@32@0:8^{__CVBuffer=}16@24
^{__CVBuffer=}16@0:8
^{__CVBuffer=}
@"NSString"
@48@0:8@16@24@32^@40
B40@0:8^{__CVBuffer=}16@24^@32
B64@0:8^{__CVBuffer=}16@24@32@40@48^@56
v56@0:8@16@24@32@40@48
{CGSize=dd}16@0:8
[91d]
[6[6{CGSize="width"d"height"d}]]
[6Q]
@"MLModel"
@"NSArray"
@"HMINMSConfiguration"
@"MLPredictionOptions"
{CGSize="width"d"height"d}
q16@0:8
v24@0:8q16
@"HMISystemResourceUsage"16@0:8
@"<HMISystemResourceUsageMonitorDelegate>"16@0:8
v24@0:8@"<HMISystemResourceUsageMonitorDelegate>"16
@"HMISystemResourceUsageMonitorImpl"
@"NSObject<OS_dispatch_queue>"
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80
@"HMIFaceRecognition"
@"NSNumber"
I16@0:8
v24@0:8Q16
@24@0:8Q16
@"NSData"
{?=qiIq}16@0:8
@48@0:8@16{?=qiIq}24
{?="value"q"timescale"i"flags"I"epoch"q}
@24@0:8q16
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
@64@0:8{?={?=qiIq}{?=qiIq}}16
@"HMFUnfairLock"
@48@0:8{?=qiIq}16@40
@"NSDate"
v48@0:8@16{?=qiIq}24
@40@0:8{?=qiIq}16
d40@0:8{?=qiIq}16
@"HMIVideoEventBuffer"
v40@0:8{?=qiIq}16
d16@0:8
@"HMIVideoTimeline"
@"HMITimeIntervalAverage"
v24@0:8d16
d32@0:8d16d24
@"MovingAverage"
v24@0:8@"HMFTimer"16
@32@0:8@16@24
@"<HMIHomePersonManagerDataSource>"
@"HMIHomePersonManagerSettings"
@"NSOperationQueue"
@"HMFTimer"
@"NSMutableDictionary"
@"HMIFaceCrop"
@"HMIFaceprint"
@24@0:8^{__CVBuffer=}16
@64@0:8^{__CVBuffer=}16{?=qiIq}24Q48Q56
@80@0:8@16{?=qiIq}24Q48Q56{CGSize=dd}64
@28@0:8f16^{CGSize=dd}20
^{__CVBuffer=}64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
^{__CVBuffer=}48@0:8^{__CVBuffer=}16{CGSize=dd}24^@40
^{__CVBuffer=}60@0:8^{__CVBuffer=}16{CGSize=dd}24I40q44^@52
^{__CVBuffer=}44@0:8^{__CVBuffer=}16I24q28^@36
^{__CVBuffer=}92@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76^@84
@40@0:8^{__CVBuffer=}16f24f28^@32
^{__CVBuffer=}32@0:8^{CGDataProvider=}16^@24
^{__CVBuffer=}32@0:8@16^@24
^{__CVBuffer=}100@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76@84^@92
^{__CVBuffer=}40@0:8{CGSize=dd}16I32B36
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48{CGSize=dd}64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48f64
B48@0:8@16@24@32^@40
^{__CVBuffer=}92@0:8^{__CVBuffer=}16f24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60Q76^@84
@24@0:8^{__IOHIDServiceClient=}16
^{__IOHIDServiceClient=}
B28@0:8i16^d20
B24@0:8^d16
^{__IOHIDEventSystemClient=}
@32@0:8@16B24B28
B32@0:8@16B24B28
^{opaqueCMSampleBuffer=}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
^{opaqueCMSampleBuffer=}24@0:8^Q16
@"AVAsset"
@"AVAssetReader"
^{__CFArray=}
v32@0:8@16Q24
v32@0:8@16@24
v32@0:8@"HMHomeManager"16Q24
v24@0:8@"HMHomeManager"16
v32@0:8@"HMHomeManager"16@"HMHome"24
v32@0:8@"HMHomeManager"16@"HMAddAccessoryRequest"24
@?16@0:8
@28@0:8@16B24
@"NSUUID"
@"MLMultiArray"
@40@0:8@16@24^@32
B40@0:8@16^d24^@32
@"NSError"
v24@0:8^{opaqueCMSampleBuffer=}16
@"HMIDESBackgroundTask"
@"HMPhotosPersonManager"
@32@0:8q16@24
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@80@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72
@24@0:8#16
@"HMIVideoFrame"
@"NSSet"
B32@0:8@16@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
q24@0:8@16
@96@0:8q16@24@32@40{?=qiIq}48@72q80Q88
@80@0:8@16@24{?=qiIq}32@56q64Q72
q24@0:8q16
B28@0:8@16B24
f16@0:8
v20@0:8f16
@"NSDictionary"
@"HMICameraVideoFragment"
@52@0:8Q16Q24d32d40B48
@60@0:8Q16Q24d32d40B48@52
v20@0:8B16
@"HMICamera"
@40@0:8q16@24@32
@"HMICameraVideoFrame"
@88@0:8@16q24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48@80
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@40@0:8Q16@24@32
@56@0:8Q16@24@32q40@48
@48@0:8Q16@24@32q40
@40@0:8Q16@24q32
@56@0:8Q16@24q32@40@48
@"NSURL"
@40@0:8q16q24@32
v40@0:8@16@24@32
@"HMICameraVideoAnalyzerResult"
@"HMICameraVideoAnalyzer"
v32@0:8@16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16@"NSError"24
v32@0:8@"HMIVideoRetimer"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoFrameSampler"16^{opaqueCMSampleBuffer=}24
v72@0:8@16@24{?=qiIq}32{CGSize=dd}56
v32@0:8@"HMICameraVideoFrameSelector"16@"HMICameraVideoFrame"24
v72@0:8@"HMICameraVideoFrameSelector"16@"NSArray"24{?=qiIq}32{CGSize=dd}56
@40@0:8@16@24d32
B32@0:8@16^@24
B32@0:8Q16Q24
@"HMICameraVideoResourceAttributes"
@"HMIVideoEncoder"
@"HMIVideoRetimer"
@"HMIVideoFrameSampler"
@"HMIVideoAssetWriter"
@"HMICameraVideoPosterFrameGenerator"
@"HMICameraVideoFrameSelector"
@"HMICameraVideoAssetReader"
v32@0:8@16q24
@"NSPointerArray"
@"HMISystemResourceUsageMonitor"
q40@0:8q16q24@32
v28@0:8@16B24
v40@0:8@16q24@32
v32@0:8q16@24
v40@0:8q16@24@32
v48@0:8q16@24@32@40
B32@0:8@16^q24
v48@0:8@16@24@32@40
@40@0:8@16@24@32
B48@0:8@16@24^@32^@40
v48@0:8@16q24@32@40
[7i]
[3i]
@"<HMICameraVideoAnalyzerDelegate>"
@"HMIHomePersonManager"
@"HMICameraVideoAnalyzerHistory"
@"HMIVideoAnalyzer"
@"HMICameraVideoAnalyzerRequest"
@"HMICameraVideoAnalyzerScheduler"
@"HMICameraVideoAnalyzerConfiguration"
@"HMIAnalysisService"
@32@0:8r^f16Q24
v32@0:8r^f16Q24
r^f16@0:8
^f16@0:8
@20@0:8f16
@"NSMutableData"
@52@0:8@16^{__CVBuffer=}24B32@36^@44
@"HMIVideoAnalyzerEventFace"52@0:8@"HMIVideoAnalyzerEventFace"16^{__CVBuffer=}24B32@"NSUUID"36^@44
@24@0:8^@16
@56@0:8d16d24d32d40^@48
@"HMIFaceprinter"
@"HMIFaceQualityFilterSVM"
B24@0:8^@16
@60@0:8@16q24B32@36@44^@52
v72@0:8@16{?=qiIq}24{CGSize=dd}48@64
@32@0:8@16d24
@"<HMICameraVideoFrameAnalyzer>"
@"HMICIFilterAttributeValue"
^{__CVBuffer=}24@0:8^@16
@56@0:8@16@24@32@40@48
@32@0:8Q16^@24
@48@0:8@16@24@32@40
@76@0:8i16d20{CGRect={CGPoint=dd}{CGSize=dd}}28@60@68
i16@0:8
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
v48@0:8@16@24d32q40
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56
B32@0:8^{opaqueCMSampleBuffer=}16@24
@"<HMIVideoFrameAnalyzerDelegate>"
v40@0:8@"HMIVideoFrameAnalyzer"16@"HMIVideoFrameAnalyzerResult"24@"NSError"32
@64@0:8@16{?=qiIq}24Q48Q56
@48@0:8{?=qiIq}16Q40
@"HMICameraVideoPosterFrameGeneratorInput"
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
@72@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{opaqueCMSampleBuffer=}
@"<HMIVideoRetimerDelegate>"
@20@0:8i16
@28@0:8i16d20
@36@0:8i16@20d28
i32@0:8@16@?24
B20@0:8i16
@32@0:8@16^@24
v20@0:8i16
@"NSArray"16@0:8
@"<HMIPersonManagerDataSource>"
@28@0:8@16f24
v32@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24
v40@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24@"HMICameraVideoFrame"32
@"<HMICameraVideoFrameSelectorDelegate>"
@"HMICameraVideoFrameSampler"
@"<HMIMotionDetector>"
@72@0:8@16{?=qiIq}24{?=qiIq}48
@"<HMICameraVideoFrameSamplerDelegate>"
q20@0:8i16
@36@0:8^{__CVBuffer=}16B24^@28
^{__CVBuffer=}48@0:8@16^{__CVBuffer=}24@32^@40
@32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8@"NSData"16^@24
@32@0:8{CGPoint=dd}16
{CGPoint=dd}16@0:8
{CGPoint="x"d"y"d}
B40@0:8@16@24^@32
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@"HMIHomeKitClient"
@"NSURLSession"
@"HMIFeedbackSession"
@36@0:8i16@20@28
@"HMFOperation"
v44@0:8@16@24B32@?36
@56@0:8@16@24@32d40q48
@72@0:8@16@24@32@40@48d56q64
@96@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72@80d88
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@28@0:8i16@20
@"HMIPersonFaceCrop"
@32@0:8d16@24
@56@0:8Q16@24@32@40@48
@48@0:8q16@24@32@40
@72@0:8@16{?={?=qiIq}{?=qiIq}}24
@80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
@88@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80
{?={?=qiIq}{?=qiIq}}16@0:8
r^{opaqueCMFormatDescription=}16@0:8
r^{opaqueCMFormatDescription=}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@32@0:8{CGSize=dd}16
@"HMIFaceTracker"
@24@0:8^{opaqueCMSampleBuffer=}16
r^{__CTFont=}24@0:8d16
^{CGContext=}
^{CGColorSpace=}
^{__CTFont=}
@"<HMISystemResourceUsageMonitorDelegate>"
@"<HMISystemResourceUsageMonitorProtocol>"
@36@0:8@16B24@28
@44@0:8@16@24@32B40
@"VNPersonsModel"
@32@0:8@16q24
@36@0:8@16B24q28
v24@0:8#16
v28@0:8i16@20
v32@0:8d16@24
v28@0:8B16@20
i24@0:8@16
d24@0:8@16
@32@0:8#16@24
{CGRect={CGPoint=dd}{CGSize=dd}}48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B64@0:8@16@24Q32@40@48^@56
@84@0:8@16@24i32{CGRect={CGPoint=dd}{CGSize=dd}}36{CGSize=dd}68
@76@0:8@16i24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60
v108@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48d80d88@96B104
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
v32@0:8{CGPoint=dd}16
{CGVector=dd}16@0:8
v32@0:8{CGVector=dd}16
{CGVector="dx"d"dy"d}
@40@0:8Q16Q24d32
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
@72@0:8@16@24@32@40{?=qiIq}48
@64@0:8{?=qiIq}16@40Q48Q56
@112@0:8{?=qiIq}16@40Q48Q56d64{CGSize=dd}72{?=qiIq}88
Q24@0:8@16
B32@0:8^@16^@24
@40@0:8{?=ii}16I24B28^@32
B28@0:8^{opaqueCMSampleBuffer=}16B24
^{OpaqueVTCompressionSession=}
{?="width"i"height"i}
@"<HMIVideoEncoderDelegate>"
v24@0:8@?<v@?@"HMIHomePersonManagerSettings"@"NSError">16
v40@0:8@"NSSet"16@"NSUUID"24@?<v@?@"NSError">32
@"HMHomePersonManager"
q32@0:8q16@24
@40@0:8@16Q24Q32
B52@0:8@16@24B32@36^@44
@44@0:8@16B24@28^@36
@72@0:8@16Q24@32@40@48@56^@64
@"HMIDESDataset"
@32@0:8Q16@24
@"<HMIExternalPersonManagerDataSource>"
@"HMIExternalPersonManagerSettings"
@36@0:8@16@24B32
@"TRIClient"
B40@0:8@16^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24^@32
f32@0:8@16@24
@56@0:8@16@24@32q40^@48
@24@0:8^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}16
{shared_ptr<homeai::clustering::GreedyClusterer>="__ptr_"^{GreedyClusterer}"__cntrl_"^{__shared_weak_count}}
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"HMICameraVideoAnalyzerResult"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"NSError"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoAnalyzerSignificantEvent"24@"HMICameraVideoFragment"32
@"NSObject<OS_dispatch_group>"
@"<HMIVideoFrameSamplerDelegate>"
@"<HMIVideoPackageAnalyzerDelegate>"
v40@0:8@"HMIVideoPackageAnalyzer"16@24@"NSError"32
f24@0:8^{__CVBuffer=}16
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
f24@0:8@16
d48@0:8d16d24d32d40
@64@0:8@16@24@32@40@48@56
@"HMIVideoFragment"
@"HMIVideoAnalyzerResultOutcome"
@"HMIVideoAnalyzerDynamicConfiguration"
@48@0:8@16@24@32f40f44
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
B72@0:8@16@24@32@40@48@56@64
B64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48#56
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
i40@0:8^{__CVBuffer=}16@24@?32
i40@0:8@16@24@?32
@"NSMapTable"
@60@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24B56
@68@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24B56@60
@"HMIVideoAnalyzerEventFace"
@60@0:8@16B24d28Q36@44^@52
@40@0:8@16@24@?32
@"HMIPerson"
@40@0:8r*16@24@?32
r*16@0:8
@60@0:8i16@20@28@36@44^@52
@"HMIGreedyClustering"
@"<HMIFaceClassifier>"
@"HMIPersonsModelManager"
@"HMIClusteringTaskSummary"
@52@0:8i16@20@28@36B44B48
@40@0:8@16{CGSize=dd}24
@56@0:8@16@24@32@40^@48
@56@0:8@"NSDictionary"16@"NSDictionary"24@"HMINMSConfiguration"32@"NSString"40^@48
v24@0:8@"HMICameraVideoFrame"16
v72@0:8@"NSArray"16{?=qiIq}24{CGSize=dd}48@"NSUUID"64
@"HMICameraVideoFrameResult"60@0:8@"HMICameraVideoFrame"16q24B32@"NSUUID"36@"NSUUID"44^@52
@"NSDictionary"16@0:8
q24@0:8#16
@28@0:8q16B24
@"HMISignificantActivityDetector"
@"HMIFaceClassifierVIP"
@"HMFOSTransaction"
@"NSCache"
@64@0:8@16{CGSize=dd}24{?=qiIq}40
@48@0:8^{__CVBuffer=}16{?=qiIq}24
@40@0:8d16d24^@32
*40@0:8Q16Q24^i32
i40@0:8^v16Q24r*32
i32@0:8@16@24
@"NSObject<OS_dispatch_semaphore>"
@"<HMIVideoCommandBufferDelegate>"
v32@0:8@"HMIVideoCommandBuffer"16^{opaqueCMSampleBuffer=}24
v24@0:8@"HMIVideoCommandBuffer"16
@"HMIConfidence"
B24@0:8r^{opaqueCMFormatDescription=}16
^{OpaqueVTDecompressionSession=}
^{opaqueCMBufferQueue=}
@"<HMIVideoDecoderDelegate>"
v32@0:8@"HMIVideoDecoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoDecoder"16@"NSError"24
v48@0:8@16@24q32@40
v40@0:8@16@24q32
v48@0:8@"AVAssetWriter"16@"NSData"24q32@"AVAssetSegmentReport"40
v40@0:8@"AVAssetWriter"16@"NSData"24q32
@32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
@"AVAssetWriter"
^{opaqueCMFormatDescription=}
@"<HMIVideoAssetWriterDelegate>"
v80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSData"24
v80@0:8@"HMIVideoAssetWriter"16@"NSData"24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSError"24
B48@0:8@16@24d32d40
@48@0:8{CGPoint=dd}16{CGVector=dd}32
@72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64
f68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
v48@0:8^{__CVBuffer=}16{?=qiIq}24
@24@0:8^f16
@"NSArray"24@0:8^f16
@40@0:8^{__CVBuffer=}16^{__CVBuffer=}24^f32
@36@0:8^{opaqueCMSampleBuffer=}16f24@28
v24@0:8^{__CVBuffer=}16
v32@0:8^{opaqueCMSampleBuffer=}16^{opaqueCMSampleBuffer=}24
@"<HMIVideoFrameSelectorDelegate>"
v40@0:8@16^{opaqueCMSampleBuffer=}24@32
v40@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24@"NSArray"32
@64@0:8@16@24@32@40d48q56
@48@0:8@16@24B32B36^@40
@"<HMIVideoAnalyzerDelegate>"
@"HMIVideoAnalyzerConfiguration"
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFragmentResult"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFrameResult"24
v32@0:8@"HMIVideoAnalyzer"16@"NSError"24
v32@0:8@"HMIVideoAnalyzer"16@"NSDictionary"24
v32@0:8^{opaqueCMSampleBuffer=}16@?24
v32@0:8:16@24
@"HMIVideoCommandBuffer"
@"HMIVideoDecoder"
@"HMIVideoFrameSelector"
@"HMIVideoFrameAnalyzer"
@"HMIVideoPackageAnalyzer"
@"HMIVideoAnalyzerScheduler"
gepj
gepj
ARGB
f024
v024
v024
333333
333333
333333
333333
